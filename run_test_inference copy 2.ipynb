{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"ganga4364/garchen_rinpoche_inference_transcripts\")\n",
    "\n",
    "test_df = dataset[\"train\"].to_pandas()\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset, Audio\n",
    "import logging\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset, Audio\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='failed_downloads.log', level=logging.ERROR, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Initialize generator pipelines\n",
    "generator1 = pipeline(task=\"automatic-speech-recognition\", model=\"ganga4364/mms_300_Garchen_Rinpoche-v5-base-Checkpoint-28000\", device=0)\n",
    "generator2 = pipeline(task=\"automatic-speech-recognition\", model=\"ganga4364/mms_300_Garchen_Rinpoche-v6-ft-Checkpoint-25000\", device=0)\n",
    "generator3 = pipeline(task=\"automatic-speech-recognition\", model=\"ganga4364/mms_300_Garchen_Rinpoche-v7-scracth-Checkpoint-23000\", device=0)\n",
    "\n",
    "# Function to download and validate audio\n",
    "def download_audio(row):\n",
    "    file_name = os.path.basename(row[\"url\"])\n",
    "    save_path = f\"./downloads/{file_name}\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(save_path):  # Skip if file exists\n",
    "        return save_path\n",
    "\n",
    "    try:\n",
    "        response = requests.get(row[\"url\"], timeout=10)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return save_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {row['file_name']}: {e}\")\n",
    "        return None  # Return None if download failed\n",
    "\n",
    "# Function to process inference in batches\n",
    "def process_inference(batch):\n",
    "    # Perform inference with generator1\n",
    "    results1 = generator1(batch[\"path\"])\n",
    "    batch[\"ft_v5_base_inference_checkpoint-28000\"] = [result[\"text\"] for result in results1]\n",
    "\n",
    "    # Perform inference with generator2\n",
    "    results2 = generator2(batch[\"path\"])\n",
    "    batch[\"ft_v6_ft_inference_checkpoint-25000\"] = [result[\"text\"] for result in results2]\n",
    "\n",
    "    # Perform inference with generator3\n",
    "    results3 = generator3(batch[\"path\"])\n",
    "    batch[\"ft_v7_scratch_inference_checkpoint-23000\"] = [result[\"text\"] for result in results3]\n",
    "\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to test file\n",
    "input_file = \"test.csv\"\n",
    "output_dir = \"chunks_test\"\n",
    "output_file = \"processed_test.csv\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, chunk in enumerate(pd.read_csv(input_file, chunksize=1000), start=1):\n",
    "    if chunk.empty:\n",
    "        print(f\"Chunk {i} is empty. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    chunk_file = os.path.join(output_dir, f\"chunk_{i}.csv\")\n",
    "    if os.path.exists(chunk_file):\n",
    "        print(f\"Skipping chunk {i}, already processed.\")\n",
    "        continue\n",
    "\n",
    "    tqdm.pandas(desc=\"Downloading audio files\")\n",
    "    from pandarallel import pandarallel\n",
    "    # Initialize pandarallel (do this once at the start of your script)\n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=4)\n",
    "    # Then replace your apply with parallel_apply\n",
    "    chunk[\"path\"] = chunk.parallel_apply(download_audio, axis=1)\n",
    "    chunk = chunk[chunk[\"path\"].notnull()]  # Remove rows with failed downloads\n",
    "\n",
    "    # Reset index to avoid duplicate field errors\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert to Dataset\n",
    "    dataset = Dataset.from_pandas(chunk)\n",
    "    #dataset = dataset.cast_column(\"path\", Audio())\n",
    "\n",
    "    # Perform batched inference\n",
    "    dataset = dataset.map(process_inference, batched=True, batch_size=8)\n",
    "\n",
    "    # Save processed chunk to CSV\n",
    "    dataset.to_pandas().to_csv(chunk_file, index=False)\n",
    "    print(f\"Saved chunk {i} to {chunk_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed chunk to CSV\n",
    "dataset.to_pandas().to_csv(chunk_file, index=False)\n",
    "print(f\"Saved chunk {i} to {chunk_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, chunk in enumerate(pd.read_csv(input_file, chunksize=1000), start=1):\n",
    "    if chunk.empty:\n",
    "        print(f\"Chunk {i} is empty. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    chunk_file = os.path.join(output_dir, f\"chunk_{i}.csv\")\n",
    "    if os.path.exists(chunk_file):\n",
    "        print(f\"Skipping chunk {i}, already processed.\")\n",
    "        continue\n",
    "\n",
    "    tqdm.pandas(desc=\"Downloading audio files\")\n",
    "    chunk[\"path\"] = chunk.progress_apply(download_audio, axis=1)\n",
    "    chunk = chunk[chunk[\"path\"].notnull()]  # Remove rows with failed downloads\n",
    "\n",
    "    # Reset index to avoid duplicate field errors\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert to Dataset\n",
    "    dataset = Dataset.from_pandas(chunk)\n",
    "    #dataset = dataset.cast_column(\"path\", Audio())\n",
    "\n",
    "    # Perform batched inference\n",
    "    dataset = dataset.map(process_inference, batched=True, batch_size=8)\n",
    "\n",
    "    # Save processed chunk to CSV\n",
    "    dataset.to_pandas().to_csv(chunk_file, index=False)\n",
    "    print(f\"Saved chunk {i} to {chunk_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all chunk files into final output\n",
    "all_chunks = [pd.read_csv(os.path.join(output_dir, f)) for f in sorted(os.listdir(output_dir)) if f.endswith(\".csv\")]\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"All chunks merged and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas evaluate jiwer \n",
    "! pip install tibetan_wer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = \"/home/gangagyatso/Downloads/stt-wav2vec2-single-speaker/merged_df.csv\"  # Update with your actual file path\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>path</th>\n",
       "      <th>url</th>\n",
       "      <th>audio_duration</th>\n",
       "      <th>original_id</th>\n",
       "      <th>strata</th>\n",
       "      <th>age_group</th>\n",
       "      <th>duration_category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>uni</th>\n",
       "      <th>...</th>\n",
       "      <th>ft_v8_scartch_inference_checkpoint-59000</th>\n",
       "      <th>ft_v8_scratch_cer_59000</th>\n",
       "      <th>ft_v8_scratch_lev_dist_59000</th>\n",
       "      <th>whisper-small-latin-added-tibetan-checkpoint-4000</th>\n",
       "      <th>whisper-small-tibetan-wylie-checkpoint-4000</th>\n",
       "      <th>whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan</th>\n",
       "      <th>whisper-small-latin-added-tibetan-checkpoint-4000_cer</th>\n",
       "      <th>whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer</th>\n",
       "      <th>whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist</th>\n",
       "      <th>whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STT_GR_0001_0010_93600_to_106200</td>\n",
       "      <td>./downloads/STT_GR_0001_0010_93600_to_106200.wav</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>12.600</td>\n",
       "      <td>STT_GR_0001</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>ཁོང་དེ་ནས་དགེ་འདུན་ཁྲི་ལ་སྔོན་ཟླ་རེ་བོད་གླེང་ད...</td>\n",
       "      <td>...</td>\n",
       "      <td>ཁོང་་དེ་ས་དགེ་འདུལ་ཁྲིི་ལང་བདག་རེད། བུ་གླིང་དེ...</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>54</td>\n",
       "      <td>ཁོང་དེ་ནས་དགེ་དུས་འཕྲལ་པའི་ལྔ་ཟླ་རེད། དེ་བུ་གླ...</td>\n",
       "      <td>khong de nas dge du khri ba'i ngab da re yin ...</td>\n",
       "      <td>ཁོང་དེ་ནས་དགེ་དུ་ཁྲི་བའི་ངབ་ད་རེ་ཡིན་པོཨོན་གླི...</td>\n",
       "      <td>0.396648</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STT_GR_0033_0105_224320_to_225856</td>\n",
       "      <td>./downloads/STT_GR_0033_0105_224320_to_225856.wav</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>1.536</td>\n",
       "      <td>STT_GR_0033</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>ཕ་གི་ཕར་རང་རེ་རྫས་ཚར་ལེ།</td>\n",
       "      <td>...</td>\n",
       "      <td>ཨ། ཕན་པ་ཡོད་ལ་དེ་ལྟ་ལུ།</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>18</td>\n",
       "      <td>ཨ། ཕན་པ་རེད་གོ་ན་དེ་ལྟར་ནས།</td>\n",
       "      <td>phan phan ba lag gi yod na de lta bu la</td>\n",
       "      <td>ཕན་ཕན་བ་ལག་གི་ཡོད་ན་དེ་ལྟ་བུ་ལ</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file_name  \\\n",
       "0   STT_GR_0001_0010_93600_to_106200   \n",
       "1  STT_GR_0033_0105_224320_to_225856   \n",
       "\n",
       "                                                path  \\\n",
       "0   ./downloads/STT_GR_0001_0010_93600_to_106200.wav   \n",
       "1  ./downloads/STT_GR_0033_0105_224320_to_225856.wav   \n",
       "\n",
       "                                                 url  audio_duration  \\\n",
       "0  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...          12.600   \n",
       "1  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...           1.536   \n",
       "\n",
       "   original_id                  strata age_group duration_category  \\\n",
       "0  STT_GR_0001  70-80__short__Teaching     70-80             short   \n",
       "1  STT_GR_0033  70-80__short__Teaching     70-80             short   \n",
       "\n",
       "  content_type                                                uni  ...  \\\n",
       "0     Teaching  ཁོང་དེ་ནས་དགེ་འདུན་ཁྲི་ལ་སྔོན་ཟླ་རེ་བོད་གླེང་ད...  ...   \n",
       "1     Teaching                           ཕ་གི་ཕར་རང་རེ་རྫས་ཚར་ལེ།  ...   \n",
       "\n",
       "            ft_v8_scartch_inference_checkpoint-59000  ft_v8_scratch_cer_59000  \\\n",
       "0  ཁོང་་དེ་ས་དགེ་འདུལ་ཁྲིི་ལང་བདག་རེད། བུ་གླིང་དེ...                 0.290503   \n",
       "1                            ཨ། ཕན་པ་ཡོད་ལ་དེ་ལྟ་ལུ།                 0.750000   \n",
       "\n",
       "   ft_v8_scratch_lev_dist_59000  \\\n",
       "0                            54   \n",
       "1                            18   \n",
       "\n",
       "   whisper-small-latin-added-tibetan-checkpoint-4000  \\\n",
       "0  ཁོང་དེ་ནས་དགེ་དུས་འཕྲལ་པའི་ལྔ་ཟླ་རེད། དེ་བུ་གླ...   \n",
       "1                        ཨ། ཕན་པ་རེད་གོ་ན་དེ་ལྟར་ནས།   \n",
       "\n",
       "         whisper-small-tibetan-wylie-checkpoint-4000  \\\n",
       "0   khong de nas dge du khri ba'i ngab da re yin ...   \n",
       "1            phan phan ba lag gi yod na de lta bu la   \n",
       "\n",
       "   whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan  \\\n",
       "0  ཁོང་དེ་ནས་དགེ་དུ་ཁྲི་བའི་ངབ་ད་རེ་ཡིན་པོཨོན་གླི...        \n",
       "1                     ཕན་ཕན་བ་ལག་གི་ཡོད་ན་དེ་ལྟ་བུ་ལ        \n",
       "\n",
       "  whisper-small-latin-added-tibetan-checkpoint-4000_cer  \\\n",
       "0                                           0.396648      \n",
       "1                                           0.791667      \n",
       "\n",
       "   whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer  \\\n",
       "0                                           0.374302            \n",
       "1                                           0.916667            \n",
       "\n",
       "   whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist  \\\n",
       "0                                                 73            \n",
       "1                                                 19            \n",
       "\n",
       "  whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist  \n",
       "0                                                 69               \n",
       "1                                                 22               \n",
       "\n",
       "[2 rows x 65 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_name', 'path', 'url', 'audio_duration', 'original_id', 'strata',\n",
       "       'age_group', 'duration_category', 'content_type', 'uni',\n",
       "       'base_transcript', 'base_cer', 'base_lev_dist',\n",
       "       'ft_v1_inference_checkpoint-5000', 'ft_v1_cer_5000',\n",
       "       'ft_v1_lev_dist_5000', 'ft_v1_inference_checkpoint-10000',\n",
       "       'ft_v1_cer_10000', 'ft_v1_lev_dist_10000',\n",
       "       'ft_v1_inference_checkpoint-19000', 'ft_v1_cer_19000',\n",
       "       'ft_v1_lev_dist_19000', 'ft_v2_inference_checkpoint-20000',\n",
       "       'ft_v2_cer_20000', 'ft_v2_lev_dist_20000',\n",
       "       'ft_v2_inference_checkpoint-32000', 'ft_v2_cer_32000',\n",
       "       'ft_v2_lev_dist_32000', 'ft_v2_inference_checkpoint-43000',\n",
       "       'ft_v2_cer_43000', 'ft_v2_lev_dist_43000',\n",
       "       'ft_v3_inference_checkpoint-1000', 'ft_v3_cer_1000',\n",
       "       'ft_v3_lev_dist_1000', 'ft_v3_inference_checkpoint-25000',\n",
       "       'ft_v3_cer_25000', 'ft_v3_lev_dist_25000',\n",
       "       'ft_v4_inference_checkpoint-22000', 'ft_v4_cer_22000',\n",
       "       'ft_v4_lev_dist_22000', 'ft_v5_base_inference_checkpoint-28000',\n",
       "       'ft_v5_base_cer_28000', 'ft_v5_base_lev_dist_28000',\n",
       "       'ft_v6_ft_inference_checkpoint-25000', 'ft_v6_ft_cer_25000',\n",
       "       'ft_v6_ft_lev_dist_25000', 'ft_v7_scratch_inference_checkpoint-23000',\n",
       "       'ft_v7_scratch_cer_23000', 'ft_v7_scratch_lev_dist_23000',\n",
       "       'ft_v8_scartch_inference_checkpoint-10000', 'ft_v8_scratch_cer_10000',\n",
       "       'ft_v8_scratch_lev_dist_10000',\n",
       "       'ft_v8_scartch_inference_checkpoint-55000', 'ft_v8_scratch_cer_55000',\n",
       "       'ft_v8_scratch_lev_dist_55000',\n",
       "       'ft_v8_scartch_inference_checkpoint-59000', 'ft_v8_scratch_cer_59000',\n",
       "       'ft_v8_scratch_lev_dist_59000', 'tibetan_transcript',\n",
       "       'wylie_transcript', 'wylie_to_tibetan_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file_name', 'path', 'url', 'audio_duration', 'original_id', 'strata', 'age_group', 'duration_category', 'content_type', 'uni', 'base_transcript', 'base_cer', 'base_lev_dist', 'ft_v1_inference_checkpoint-5000', 'ft_v1_cer_5000', 'ft_v1_lev_dist_5000', 'ft_v1_inference_checkpoint-10000', 'ft_v1_cer_10000', 'ft_v1_lev_dist_10000', 'ft_v1_inference_checkpoint-19000', 'ft_v1_cer_19000', 'ft_v1_lev_dist_19000', 'ft_v2_inference_checkpoint-20000', 'ft_v2_cer_20000', 'ft_v2_lev_dist_20000', 'ft_v2_inference_checkpoint-32000', 'ft_v2_cer_32000', 'ft_v2_lev_dist_32000', 'ft_v2_inference_checkpoint-43000', 'ft_v2_cer_43000', 'ft_v2_lev_dist_43000', 'ft_v3_inference_checkpoint-1000', 'ft_v3_cer_1000', 'ft_v3_lev_dist_1000', 'ft_v3_inference_checkpoint-25000', 'ft_v3_cer_25000', 'ft_v3_lev_dist_25000', 'ft_v4_inference_checkpoint-22000', 'ft_v4_cer_22000', 'ft_v4_lev_dist_22000', 'ft_v5_base_inference_checkpoint-28000', 'ft_v5_base_cer_28000', 'ft_v5_base_lev_dist_28000', 'ft_v6_ft_inference_checkpoint-25000', 'ft_v6_ft_cer_25000', 'ft_v6_ft_lev_dist_25000', 'ft_v7_scratch_inference_checkpoint-23000', 'ft_v7_scratch_cer_23000', 'ft_v7_scratch_lev_dist_23000', 'ft_v8_scartch_inference_checkpoint-10000', 'ft_v8_scratch_cer_10000', 'ft_v8_scratch_lev_dist_10000', 'ft_v8_scartch_inference_checkpoint-55000', 'ft_v8_scratch_cer_55000', 'ft_v8_scratch_lev_dist_55000', 'ft_v8_scartch_inference_checkpoint-59000', 'ft_v8_scratch_cer_59000', 'ft_v8_scratch_lev_dist_59000', 'whisper-small-latin-added-tibetan-checkpoint-4000', 'whisper-small-tibetan-wylie-checkpoint-4000', 'whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan']\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping of old column names to new column names\n",
    "column_mapping = {\n",
    "    'tibetan_transcript': 'whisper-small-latin-added-tibetan-checkpoint-4000',\n",
    "    'wylie_transcript': 'whisper-small-tibetan-wylie-checkpoint-4000',\n",
    "    'wylie_to_tibetan_text': 'whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan'\n",
    "}\n",
    "\n",
    "# Apply the rename operation\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# Verify the columns have been renamed\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating whisper-small-latin-added-tibetan-checkpoint-4000_cer...\n",
      "Calculating whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer...\n",
      "CER calculations completed and saved to processed_test_with_cer.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "# Load the CSV file\n",
    "#csv_file = \"/home/gangagyatso/Downloads/stt-wav2vec2-single-speaker/merged_df.csv\"  # Update with your actual file path\n",
    "#df = pd.read_csv(csv_file)\n",
    "\n",
    "# Define the inference columns and their corresponding CER column names\n",
    "inference_to_cer_mapping = {\n",
    "    \"whisper-small-latin-added-tibetan-checkpoint-4000\": \"whisper-small-latin-added-tibetan-checkpoint-4000_cer\",\n",
    "    \"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan\": \"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer\",\n",
    "}\n",
    "\n",
    "# Function to calculate CER\n",
    "def calculate_cer(reference, prediction):\n",
    "    try:\n",
    "        if not reference or not prediction or pd.isna(reference) or pd.isna(prediction):\n",
    "            return np.nan\n",
    "        cer = cer_metric.compute(references=[reference], predictions=[prediction])\n",
    "        return min(cer, 1.0)  # Cap at 1.0 as seen in your existing data\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating CER: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Calculate and add CER columns\n",
    "for inference_col, cer_col in inference_to_cer_mapping.items():\n",
    "    print(f\"Calculating {cer_col}...\")\n",
    "    df[cer_col] = df.apply(\n",
    "        lambda row: calculate_cer(row['uni'], row[inference_col]), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(\"processed_test_with_cer.csv\", index=False)\n",
    "print(\"CER calculations completed and saved to processed_test_with_cer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist...\n",
      "Calculating whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist...\n",
      "CER calculations completed and saved to processed_test_with_cer_and_lev_dist.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = \"processed_test_with_cer.csv\"  # Update with your actual file path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Define the inference columns and their corresponding CER column names\n",
    "inference_to_cer_mapping = {\n",
    "    \"whisper-small-latin-added-tibetan-checkpoint-4000\": \"whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist\",\n",
    "    \"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan\": \"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist\",\n",
    "\n",
    "}\n",
    "\n",
    "# Function to calculate CER\n",
    "def calculate_lev_dist(reference, prediction):\n",
    "    try:\n",
    "        if not reference or not prediction or pd.isna(reference) or pd.isna(prediction):\n",
    "            return np.nan\n",
    "        lev_dist = distance(reference, prediction)\n",
    "        return lev_dist\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating CER: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Calculate and add CER columns\n",
    "for inference_col, cer_col in inference_to_cer_mapping.items():\n",
    "    print(f\"Calculating {cer_col}...\")\n",
    "    df[cer_col] = df.apply(\n",
    "        lambda row: calculate_lev_dist(row['uni'], row[inference_col]), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(\"processed_test_with_cer_and_lev_dist.csv\", index=False)  \n",
    "print(\"CER calculations completed and saved to processed_test_with_cer_and_lev_dist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"processed_test_with_cer_and_lev_dist.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_name', 'path', 'url', 'audio_duration', 'original_id', 'strata',\n",
       "       'age_group', 'duration_category', 'content_type', 'uni',\n",
       "       'base_transcript', 'base_cer', 'base_lev_dist',\n",
       "       'ft_v1_inference_checkpoint-5000', 'ft_v1_cer_5000',\n",
       "       'ft_v1_lev_dist_5000', 'ft_v1_inference_checkpoint-10000',\n",
       "       'ft_v1_cer_10000', 'ft_v1_lev_dist_10000',\n",
       "       'ft_v1_inference_checkpoint-19000', 'ft_v1_cer_19000',\n",
       "       'ft_v1_lev_dist_19000', 'ft_v2_inference_checkpoint-20000',\n",
       "       'ft_v2_cer_20000', 'ft_v2_lev_dist_20000',\n",
       "       'ft_v2_inference_checkpoint-32000', 'ft_v2_cer_32000',\n",
       "       'ft_v2_lev_dist_32000', 'ft_v2_inference_checkpoint-43000',\n",
       "       'ft_v2_cer_43000', 'ft_v2_lev_dist_43000',\n",
       "       'ft_v3_inference_checkpoint-1000', 'ft_v3_cer_1000',\n",
       "       'ft_v3_lev_dist_1000', 'ft_v3_inference_checkpoint-25000',\n",
       "       'ft_v3_cer_25000', 'ft_v3_lev_dist_25000',\n",
       "       'ft_v4_inference_checkpoint-22000', 'ft_v4_cer_22000',\n",
       "       'ft_v4_lev_dist_22000', 'ft_v5_base_inference_checkpoint-28000',\n",
       "       'ft_v5_base_cer_28000', 'ft_v5_base_lev_dist_28000',\n",
       "       'ft_v6_ft_inference_checkpoint-25000', 'ft_v6_ft_cer_25000',\n",
       "       'ft_v6_ft_lev_dist_25000', 'ft_v7_scratch_inference_checkpoint-23000',\n",
       "       'ft_v7_scratch_cer_23000', 'ft_v7_scratch_lev_dist_23000',\n",
       "       'ft_v8_scartch_inference_checkpoint-10000', 'ft_v8_scratch_cer_10000',\n",
       "       'ft_v8_scratch_lev_dist_10000',\n",
       "       'ft_v8_scartch_inference_checkpoint-55000', 'ft_v8_scratch_cer_55000',\n",
       "       'ft_v8_scratch_lev_dist_55000',\n",
       "       'ft_v8_scartch_inference_checkpoint-59000', 'ft_v8_scratch_cer_59000',\n",
       "       'ft_v8_scratch_lev_dist_59000',\n",
       "       'whisper-small-latin-added-tibetan-checkpoint-4000',\n",
       "       'whisper-small-tibetan-wylie-checkpoint-4000',\n",
       "       'whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan',\n",
       "       'whisper-small-latin-added-tibetan-checkpoint-4000_cer',\n",
       "       'whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer',\n",
       "       'whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist',\n",
       "       'whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_cer mean: 0.2766951690814779\n",
      "ft_v1_cer_5000 mean: 0.27408927238542324\n",
      "ft_v1_cer_10000 mean: 0.23372037107449425\n",
      "ft_v1_cer_19000 mean: 0.22933289935103854\n",
      "ft_v2_cer_20000 mean: 0.22269392860735918\n",
      "ft_v2_cer_32000 mean: 0.21756439369869338\n",
      "ft_v2_cer_43000 mean: 0.2153139333090411\n",
      "ft_v3_cer_1000 mean: 0.2336476706095409\n",
      "ft_v3_cer_25000 mean: 0.22213554200186256\n",
      "ft_v4_cer_22000 mean: 0.2266093220628319\n",
      "ft_v5_base_cer_28000 mean: 0.21519361672096998\n",
      "ft_v6_ft_cer_25000 mean: 0.22091044667156381\n",
      "ft_v7_scratch_cer_23000 mean: 0.21855046572947306\n",
      "ft_v8_scratch_cer_10000 mean: 0.24240847957505046\n",
      "ft_v8_scratch_cer_55000 mean: 0.22025034419439046\n",
      "ft_v8_scratch_cer_59000 mean: 0.23043649332091357\n",
      "whisper-small-latin-added-tibetan-checkpoint-4000 cer mean: 0.29880807526513936\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan cer mean: 0.3848483478730869\n"
     ]
    }
   ],
   "source": [
    "print(\"base_cer mean:\" ,df['base_cer'].mean())\n",
    "print(\"ft_v1_cer_5000 mean:\" ,df['ft_v1_cer_5000'].mean())\n",
    "print(\"ft_v1_cer_10000 mean:\" ,df['ft_v1_cer_10000'].mean())\n",
    "print(\"ft_v1_cer_19000 mean:\" ,df['ft_v1_cer_19000'].mean())\n",
    "print(\"ft_v2_cer_20000 mean:\" ,df['ft_v2_cer_20000'].mean())\n",
    "print(\"ft_v2_cer_32000 mean:\" ,df['ft_v2_cer_32000'].mean())\n",
    "print(\"ft_v2_cer_43000 mean:\" ,df['ft_v2_cer_43000'].mean())\n",
    "print(\"ft_v3_cer_1000 mean:\" ,df['ft_v3_cer_1000'].mean())\n",
    "print(\"ft_v3_cer_25000 mean:\" ,df['ft_v3_cer_25000'].mean())\n",
    "print(\"ft_v4_cer_22000 mean:\" ,df['ft_v4_cer_22000'].mean())\n",
    "print(\"ft_v5_base_cer_28000 mean:\" ,df['ft_v5_base_cer_28000'].mean())\n",
    "print(\"ft_v6_ft_cer_25000 mean:\" ,df['ft_v6_ft_cer_25000'].mean())\n",
    "print(\"ft_v7_scratch_cer_23000 mean:\" ,df['ft_v7_scratch_cer_23000'].mean())\n",
    "print(\"ft_v8_scratch_cer_10000 mean:\" ,df['ft_v8_scratch_cer_10000'].mean())\n",
    "print(\"ft_v8_scratch_cer_55000 mean:\" ,df['ft_v8_scratch_cer_55000'].mean())\n",
    "print(\"ft_v8_scratch_cer_59000 mean:\" ,df['ft_v8_scratch_cer_59000'].mean())\n",
    "print(\"whisper-small-latin-added-tibetan-checkpoint-4000 cer mean:\" ,df['whisper-small-latin-added-tibetan-checkpoint-4000_cer'].mean())\n",
    "print(\"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan cer mean:\" ,df['whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer'].mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_lev_dist mean: 19.17245240761478\n",
      "ft_v1_lev_dist_5000 mean: 18.44120940649496\n",
      "ft_v1_lev_dist_10000 mean: 15.65845464725644\n",
      "ft_v1_lev_dist_19000 mean: 15.33370660694289\n",
      "ft_v2_lev_dist_20000 mean: 14.883538633818588\n",
      "ft_v2_lev_dist_32000 mean: 14.730123180291153\n",
      "ft_v2_lev_dist_43000 mean: 14.608062709966406\n",
      "ft_v3_lev_dist_1000 mean: 15.440089585666293\n",
      "ft_v3_lev_dist_25000 mean: 14.991041433370661\n",
      "ft_v4_lev_dist_22000 mean: 15.16909294512878\n",
      "ft_v5_base_lev_dist_28000 mean: 14.96976483762598\n",
      "ft_v6_ft_lev_dist_25000 mean: 15.213885778275476\n",
      "ft_v7_scratch_lev_dist_23000 mean: 14.57110862262038\n",
      "ft_v8_scratch_lev_dist_10000 mean: 16.829787234042552\n",
      "ft_v8_scratch_lev_dist_55000 mean: 15.21612541993281\n",
      "ft_v8_scratch_lev_dist_59000 mean: 15.858902575587907\n",
      "whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist mean: 23.25979843225084\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist mean: 29.949608062709967\n"
     ]
    }
   ],
   "source": [
    "print(\"base_lev_dist mean:\" ,df['base_lev_dist'].mean())\n",
    "print(\"ft_v1_lev_dist_5000 mean:\" ,df['ft_v1_lev_dist_5000'].mean())\n",
    "print(\"ft_v1_lev_dist_10000 mean:\" ,df['ft_v1_lev_dist_10000'].mean())\n",
    "print(\"ft_v1_lev_dist_19000 mean:\" ,df['ft_v1_lev_dist_19000'].mean())\n",
    "print(\"ft_v2_lev_dist_20000 mean:\" ,df['ft_v2_lev_dist_20000'].mean())\n",
    "print(\"ft_v2_lev_dist_32000 mean:\" ,df['ft_v2_lev_dist_32000'].mean())\n",
    "print(\"ft_v2_lev_dist_43000 mean:\" ,df['ft_v2_lev_dist_43000'].mean())\n",
    "print(\"ft_v3_lev_dist_1000 mean:\" ,df['ft_v3_lev_dist_1000'].mean())\n",
    "print(\"ft_v3_lev_dist_25000 mean:\" ,df['ft_v3_lev_dist_25000'].mean())\n",
    "print(\"ft_v4_lev_dist_22000 mean:\" ,df['ft_v4_lev_dist_22000'].mean())\n",
    "print(\"ft_v5_base_lev_dist_28000 mean:\" ,df['ft_v5_base_lev_dist_28000'].mean())\n",
    "print(\"ft_v6_ft_lev_dist_25000 mean:\" ,df['ft_v6_ft_lev_dist_25000'].mean())\n",
    "print(\"ft_v7_scratch_lev_dist_23000 mean:\" ,df['ft_v7_scratch_lev_dist_23000'].mean())\n",
    "print(\"ft_v8_scratch_lev_dist_10000 mean:\" ,df['ft_v8_scratch_lev_dist_10000'].mean())\n",
    "print(\"ft_v8_scratch_lev_dist_55000 mean:\" ,df['ft_v8_scratch_lev_dist_55000'].mean())\n",
    "print(\"ft_v8_scratch_lev_dist_59000 mean:\" ,df['ft_v8_scratch_lev_dist_59000'].mean())\n",
    "print(\"whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist mean:\" ,df['whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist'].mean())\n",
    "print(\"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist mean:\" ,df['whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist'].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_cer std: 0.16075142686673585\n",
      "ft_v1_cer_5000 std: 0.18171980976826388\n",
      "ft_v1_cer_10000 std: 0.17244792880666696\n",
      "ft_v1_cer_19000 std: 0.17190517062534671\n",
      "ft_v2_cer_20000 std: 0.17455029886076237\n",
      "ft_v2_cer_32000 std: 0.1670877953428531\n",
      "ft_v2_cer_43000 std: 0.16814154355068064\n",
      "ft_v3_cer_1000 std: 0.17568876920450874\n",
      "ft_v3_cer_25000 std: 0.17130500294831275\n",
      "ft_v4_cer_22000 std: 0.1714633815167261\n",
      "ft_v5_base_cer_28000 std: 0.16066051869804554\n",
      "ft_v6_ft_cer_25000 std: 0.16997512240083704\n",
      "ft_v7_scratch_cer_23000 std: 0.16717906434681518\n",
      "ft_v8_scratch_cer_10000 std: 0.16544477870138555\n",
      "ft_v8_scratch_cer_55000 std: 0.1637333213291196\n",
      "ft_v8_scratch_cer_59000 std: 0.1647325278426431\n",
      "whisper-small-latin-added-tibetan-checkpoint-4000_cer std: 0.17580701504497323\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer std: 0.2005481872804231\n"
     ]
    }
   ],
   "source": [
    "print(\"base_cer std:\" ,df['base_cer'].std())\n",
    "print(\"ft_v1_cer_5000 std:\" ,df['ft_v1_cer_5000'].std())\n",
    "print(\"ft_v1_cer_10000 std:\" ,df['ft_v1_cer_10000'].std())\n",
    "print(\"ft_v1_cer_19000 std:\" ,df['ft_v1_cer_19000'].std())\n",
    "print(\"ft_v2_cer_20000 std:\" ,df['ft_v2_cer_20000'].std())\n",
    "print(\"ft_v2_cer_32000 std:\" ,df['ft_v2_cer_32000'].std())\n",
    "print(\"ft_v2_cer_43000 std:\" ,df['ft_v2_cer_43000'].std())\n",
    "print(\"ft_v3_cer_1000 std:\" ,df['ft_v3_cer_1000'].std())\n",
    "print(\"ft_v3_cer_25000 std:\" ,df['ft_v3_cer_25000'].std())\n",
    "print(\"ft_v4_cer_22000 std:\" ,df['ft_v4_cer_22000'].std())\n",
    "print(\"ft_v5_base_cer_28000 std:\" ,df['ft_v5_base_cer_28000'].std())\n",
    "print(\"ft_v6_ft_cer_25000 std:\" ,df['ft_v6_ft_cer_25000'].std())\n",
    "print(\"ft_v7_scratch_cer_23000 std:\" ,df['ft_v7_scratch_cer_23000'].std())\n",
    "print(\"ft_v8_scratch_cer_10000 std:\" ,df['ft_v8_scratch_cer_10000'].std())\n",
    "print(\"ft_v8_scratch_cer_55000 std:\" ,df['ft_v8_scratch_cer_55000'].std())\n",
    "print(\"ft_v8_scratch_cer_59000 std:\" ,df['ft_v8_scratch_cer_59000'].std())\n",
    "print(\"whisper-small-latin-added-tibetan-checkpoint-4000_cer std:\" ,df['whisper-small-latin-added-tibetan-checkpoint-4000_cer'].std())\n",
    "print(\"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer std:\" ,df['whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer'].std())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_lev_dist std: 21.79365734265566\n",
      "ft_v1_lev_dist_5000 std: 19.909328195806655\n",
      "ft_v1_lev_dist_10000 std: 17.421535454982365\n",
      "ft_v1_lev_dist_19000 std: 16.93500353273463\n",
      "ft_v2_lev_dist_20000 std: 16.348704643421886\n",
      "ft_v2_lev_dist_32000 std: 16.14383080904553\n",
      "ft_v2_lev_dist_43000 std: 16.028660637297786\n",
      "ft_v3_lev_dist_1000 std: 16.87683040373015\n",
      "ft_v3_lev_dist_25000 std: 16.40497601460487\n",
      "ft_v4_lev_dist_22000 std: 16.709717204794984\n",
      "ft_v5_base_lev_dist_28000 std: 16.64493969786414\n",
      "ft_v6_ft_lev_dist_25000 std: 17.197812061550955\n",
      "ft_v7_scratch_lev_dist_23000 std: 15.466611464122776\n",
      "ft_v8_scratch_lev_dist_10000 std: 19.463030600512184\n",
      "ft_v8_scratch_lev_dist_55000 std: 17.316601064144933\n",
      "ft_v8_scratch_lev_dist_59000 std: 18.459937889008685\n",
      "whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist std: 33.61533467141793\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist std: 43.62401440369452\n"
     ]
    }
   ],
   "source": [
    "print(\"base_lev_dist std:\" ,df['base_lev_dist'].std())\n",
    "print(\"ft_v1_lev_dist_5000 std:\" ,df['ft_v1_lev_dist_5000'].std())\n",
    "print(\"ft_v1_lev_dist_10000 std:\" ,df['ft_v1_lev_dist_10000'].std())\n",
    "print(\"ft_v1_lev_dist_19000 std:\" ,df['ft_v1_lev_dist_19000'].std())\n",
    "print(\"ft_v2_lev_dist_20000 std:\" ,df['ft_v2_lev_dist_20000'].std())\n",
    "print(\"ft_v2_lev_dist_32000 std:\" ,df['ft_v2_lev_dist_32000'].std())\n",
    "print(\"ft_v2_lev_dist_43000 std:\" ,df['ft_v2_lev_dist_43000'].std())\n",
    "print(\"ft_v3_lev_dist_1000 std:\" ,df['ft_v3_lev_dist_1000'].std())\n",
    "print(\"ft_v3_lev_dist_25000 std:\" ,df['ft_v3_lev_dist_25000'].std())\n",
    "print(\"ft_v4_lev_dist_22000 std:\" ,df['ft_v4_lev_dist_22000'].std())\n",
    "print(\"ft_v5_base_lev_dist_28000 std:\" ,df['ft_v5_base_lev_dist_28000'].std())\n",
    "print(\"ft_v6_ft_lev_dist_25000 std:\" ,df['ft_v6_ft_lev_dist_25000'].std())\n",
    "print(\"ft_v7_scratch_lev_dist_23000 std:\" ,df['ft_v7_scratch_lev_dist_23000'].std())\n",
    "print(\"ft_v8_scratch_lev_dist_10000 std:\" ,df['ft_v8_scratch_lev_dist_10000'].std())\n",
    "print(\"ft_v8_scratch_lev_dist_55000 std:\" ,df['ft_v8_scratch_lev_dist_55000'].std())\n",
    "print(\"ft_v8_scratch_lev_dist_59000 std:\" ,df['ft_v8_scratch_lev_dist_59000'].std())\n",
    "print(\"whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist std:\" ,df['whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist'].std())\n",
    "print(\"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist std:\" ,df['whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist'].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_cer min: 0.0\n",
      "ft_v1_cer_5000 min: 0.0\n",
      "ft_v1_cer_10000 min: 0.0\n",
      "ft_v1_cer_19000 min: 0.0\n",
      "ft_v2_cer_20000 min: 0.0\n",
      "ft_v2_cer_32000 min: 0.0\n",
      "ft_v2_cer_43000 min: 0.0\n",
      "ft_v3_cer_1000 min: 0.0\n",
      "ft_v3_cer_25000 min: 0.0\n",
      "ft_v4_cer_22000 min: 0.0\n",
      "ft_v5_base_cer_28000 min: 0.0\n",
      "ft_v6_ft_cer_25000 min: 0.0\n",
      "ft_v7_scratch_cer_23000 min: 0.0\n",
      "ft_v8_scratch_cer_10000 min: 0.0\n",
      "ft_v8_scratch_cer_55000 min: 0.0\n",
      "ft_v8_scratch_cer_59000 min: 0.0\n",
      "whisper-small-latin-added-tibetan-checkpoint-4000_cer min: 0.0\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer min: 0.0344827586206896\n"
     ]
    }
   ],
   "source": [
    "print(\"base_cer min:\" ,df['base_cer'].min())\n",
    "print(\"ft_v1_cer_5000 min:\" ,df['ft_v1_cer_5000'].min())\n",
    "print(\"ft_v1_cer_10000 min:\" ,df['ft_v1_cer_10000'].min())  \n",
    "print(\"ft_v1_cer_19000 min:\" ,df['ft_v1_cer_19000'].min())\n",
    "print(\"ft_v2_cer_20000 min:\" ,df['ft_v2_cer_20000'].min())\n",
    "print(\"ft_v2_cer_32000 min:\" ,df['ft_v2_cer_32000'].min())\n",
    "print(\"ft_v2_cer_43000 min:\" ,df['ft_v2_cer_43000'].min())\n",
    "print(\"ft_v3_cer_1000 min:\" ,df['ft_v3_cer_1000'].min())\n",
    "print(\"ft_v3_cer_25000 min:\" ,df['ft_v3_cer_25000'].min())\n",
    "print(\"ft_v4_cer_22000 min:\" ,df['ft_v4_cer_22000'].min())\n",
    "print(\"ft_v5_base_cer_28000 min:\" ,df['ft_v5_base_cer_28000'].min())\n",
    "print(\"ft_v6_ft_cer_25000 min:\" ,df['ft_v6_ft_cer_25000'].min())\n",
    "print(\"ft_v7_scratch_cer_23000 min:\" ,df['ft_v7_scratch_cer_23000'].min())\n",
    "print(\"ft_v8_scratch_cer_10000 min:\" ,df['ft_v8_scratch_cer_10000'].min())\n",
    "print(\"ft_v8_scratch_cer_55000 min:\" ,df['ft_v8_scratch_cer_55000'].min())\n",
    "print(\"ft_v8_scratch_cer_59000 min:\" ,df['ft_v8_scratch_cer_59000'].min())\n",
    "print(\"whisper-small-latin-added-tibetan-checkpoint-4000_cer min:\" ,df['whisper-small-latin-added-tibetan-checkpoint-4000_cer'].min())\n",
    "print(\"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer min:\" ,df['whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer'].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_lev_dist min: 0\n",
      "ft_v1_lev_dist_5000 min: 0\n",
      "ft_v1_lev_dist_10000 min: 0\n",
      "ft_v1_lev_dist_19000 min: 0\n",
      "ft_v2_lev_dist_20000 min: 0\n",
      "ft_v2_lev_dist_32000 min: 0\n",
      "ft_v2_lev_dist_43000 min: 0\n",
      "ft_v3_lev_dist_1000 min: 0\n",
      "ft_v3_lev_dist_25000 min: 0\n",
      "ft_v4_lev_dist_22000 min: 0\n",
      "ft_v5_base_lev_dist_28000 min: 0\n",
      "ft_v6_ft_lev_dist_25000 min: 0\n",
      "ft_v7_scratch_lev_dist_23000 min: 0\n",
      "ft_v8_scratch_lev_dist_10000 min: 0\n",
      "ft_v8_scratch_lev_dist_55000 min: 0\n",
      "ft_v8_scratch_lev_dist_59000 min: 0\n",
      "whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist min: 0\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist min: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"base_lev_dist min:\" ,df['base_lev_dist'].min())\n",
    "print(\"ft_v1_lev_dist_5000 min:\" ,df['ft_v1_lev_dist_5000'].min())\n",
    "print(\"ft_v1_lev_dist_10000 min:\" ,df['ft_v1_lev_dist_10000'].min())\n",
    "print(\"ft_v1_lev_dist_19000 min:\" ,df['ft_v1_lev_dist_19000'].min())\n",
    "print(\"ft_v2_lev_dist_20000 min:\" ,df['ft_v2_lev_dist_20000'].min())\n",
    "print(\"ft_v2_lev_dist_32000 min:\" ,df['ft_v2_lev_dist_32000'].min())\n",
    "print(\"ft_v2_lev_dist_43000 min:\" ,df['ft_v2_lev_dist_43000'].min())\n",
    "print(\"ft_v3_lev_dist_1000 min:\" ,df['ft_v3_lev_dist_1000'].min())\n",
    "print(\"ft_v3_lev_dist_25000 min:\" ,df['ft_v3_lev_dist_25000'].min())\n",
    "print(\"ft_v4_lev_dist_22000 min:\" ,df['ft_v4_lev_dist_22000'].min())\n",
    "print(\"ft_v5_base_lev_dist_28000 min:\" ,df['ft_v5_base_lev_dist_28000'].min())\n",
    "print(\"ft_v6_ft_lev_dist_25000 min:\" ,df['ft_v6_ft_lev_dist_25000'].min())\n",
    "print(\"ft_v7_scratch_lev_dist_23000 min:\" ,df['ft_v7_scratch_lev_dist_23000'].min())\n",
    "print(\"ft_v8_scratch_lev_dist_10000 min:\" ,df['ft_v8_scratch_lev_dist_10000'].min())\n",
    "print(\"ft_v8_scratch_lev_dist_55000 min:\" ,df['ft_v8_scratch_lev_dist_55000'].min())\n",
    "print(\"ft_v8_scratch_lev_dist_59000 min:\" ,df['ft_v8_scratch_lev_dist_59000'].min())\n",
    "print(\"whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist min:\" ,df['whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist'].min())\n",
    "print(\"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist min:\" ,df['whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_cer max: 1.181818181818182\n",
      "ft_v1_cer_5000 max: 1.6666666666666667\n",
      "ft_v1_cer_10000 max: 1.0909090909090908\n",
      "ft_v1_cer_19000 max: 1.0\n",
      "ft_v2_cer_20000 max: 1.0\n",
      "ft_v2_cer_32000 max: 1.0\n",
      "ft_v2_cer_43000 max: 1.0\n",
      "ft_v3_cer_1000 max: 1.0\n",
      "ft_v3_cer_25000 max: 1.0\n",
      "ft_v4_cer_22000 max: 1.0\n",
      "ft_v5_base_cer_28000 max: 1.0\n",
      "ft_v6_ft_cer_25000 max: 1.0\n",
      "ft_v7_scratch_cer_23000 max: 1.0\n",
      "ft_v8_scratch_cer_10000 max: 0.9090909090909092\n",
      "ft_v8_scratch_cer_55000 max: 1.0\n",
      "ft_v8_scratch_cer_59000 max: 1.0\n",
      "whisper-small-latin-added-tibetan-checkpoint-4000_cer max: 1.0\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer max: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"base_cer max:\" ,df['base_cer'].max())\n",
    "print(\"ft_v1_cer_5000 max:\" ,df['ft_v1_cer_5000'].max())\n",
    "print(\"ft_v1_cer_10000 max:\" ,df['ft_v1_cer_10000'].max())  \n",
    "print(\"ft_v1_cer_19000 max:\" ,df['ft_v1_cer_19000'].max())\n",
    "print(\"ft_v2_cer_20000 max:\" ,df['ft_v2_cer_20000'].max())\n",
    "print(\"ft_v2_cer_32000 max:\" ,df['ft_v2_cer_32000'].max())\n",
    "print(\"ft_v2_cer_43000 max:\" ,df['ft_v2_cer_43000'].max())\n",
    "print(\"ft_v3_cer_1000 max:\" ,df['ft_v3_cer_1000'].max())\n",
    "print(\"ft_v3_cer_25000 max:\" ,df['ft_v3_cer_25000'].max())\n",
    "print(\"ft_v4_cer_22000 max:\" ,df['ft_v4_cer_22000'].max())\n",
    "print(\"ft_v5_base_cer_28000 max:\" ,df['ft_v5_base_cer_28000'].max())\n",
    "print(\"ft_v6_ft_cer_25000 max:\" ,df['ft_v6_ft_cer_25000'].max())\n",
    "print(\"ft_v7_scratch_cer_23000 max:\" ,df['ft_v7_scratch_cer_23000'].max())\n",
    "print(\"ft_v8_scratch_cer_10000 max:\" ,df['ft_v8_scratch_cer_10000'].max())\n",
    "print(\"ft_v8_scratch_cer_55000 max:\" ,df['ft_v8_scratch_cer_55000'].max())\n",
    "print(\"ft_v8_scratch_cer_59000 max:\" ,df['ft_v8_scratch_cer_59000'].max())\n",
    "print(\"whisper-small-latin-added-tibetan-checkpoint-4000_cer max:\" ,df['whisper-small-latin-added-tibetan-checkpoint-4000_cer'].max())\n",
    "print(\"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer max:\" ,df['whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer'].max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_lev_dist max: 134\n",
      "ft_v1_lev_dist_5000 max: 155\n",
      "ft_v1_lev_dist_10000 max: 154\n",
      "ft_v1_lev_dist_19000 max: 141\n",
      "ft_v2_lev_dist_20000 max: 130\n",
      "ft_v2_lev_dist_32000 max: 129\n",
      "ft_v2_lev_dist_43000 max: 124\n",
      "ft_v3_lev_dist_1000 max: 143\n",
      "ft_v3_lev_dist_25000 max: 128\n",
      "ft_v4_lev_dist_22000 max: 134\n",
      "ft_v5_base_lev_dist_28000 max: 149\n",
      "ft_v6_ft_lev_dist_25000 max: 135\n",
      "ft_v7_scratch_lev_dist_23000 max: 125\n",
      "ft_v8_scratch_lev_dist_10000 max: 205\n",
      "ft_v8_scratch_lev_dist_55000 max: 173\n",
      "ft_v8_scratch_lev_dist_59000 max: 185\n",
      "whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist max: 379\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist max: 526\n"
     ]
    }
   ],
   "source": [
    "print(\"base_lev_dist max:\" ,df['base_lev_dist'].max())\n",
    "print(\"ft_v1_lev_dist_5000 max:\" ,df['ft_v1_lev_dist_5000'].max())\n",
    "print(\"ft_v1_lev_dist_10000 max:\" ,df['ft_v1_lev_dist_10000'].max())\n",
    "print(\"ft_v1_lev_dist_19000 max:\" ,df['ft_v1_lev_dist_19000'].max())\n",
    "print(\"ft_v2_lev_dist_20000 max:\" ,df['ft_v2_lev_dist_20000'].max())\n",
    "print(\"ft_v2_lev_dist_32000 max:\" ,df['ft_v2_lev_dist_32000'].max())\n",
    "print(\"ft_v2_lev_dist_43000 max:\" ,df['ft_v2_lev_dist_43000'].max())\n",
    "print(\"ft_v3_lev_dist_1000 max:\" ,df['ft_v3_lev_dist_1000'].max())\n",
    "print(\"ft_v3_lev_dist_25000 max:\" ,df['ft_v3_lev_dist_25000'].max())\n",
    "print(\"ft_v4_lev_dist_22000 max:\" ,df['ft_v4_lev_dist_22000'].max())\n",
    "print(\"ft_v5_base_lev_dist_28000 max:\" ,df['ft_v5_base_lev_dist_28000'].max())\n",
    "print(\"ft_v6_ft_lev_dist_25000 max:\" ,df['ft_v6_ft_lev_dist_25000'].max())\n",
    "print(\"ft_v7_scratch_lev_dist_23000 max:\" ,df['ft_v7_scratch_lev_dist_23000'].max())\n",
    "print(\"ft_v8_scratch_lev_dist_10000 max:\" ,df['ft_v8_scratch_lev_dist_10000'].max())\n",
    "print(\"ft_v8_scratch_lev_dist_55000 max:\" ,df['ft_v8_scratch_lev_dist_55000'].max())\n",
    "print(\"ft_v8_scratch_lev_dist_59000 max:\" ,df['ft_v8_scratch_lev_dist_59000'].max())\n",
    "print(\"whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist max:\" ,df['whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist'].max())\n",
    "print(\"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist max:\" ,df['whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist'].max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        mean       std\n",
      "base_cer                                            0.276695  0.160751\n",
      "ft_v1_cer_5000                                      0.274089  0.181720\n",
      "ft_v1_cer_10000                                     0.233720  0.172448\n",
      "ft_v1_cer_19000                                     0.229333  0.171905\n",
      "ft_v2_cer_20000                                     0.222694  0.174550\n",
      "ft_v2_cer_32000                                     0.217564  0.167088\n",
      "ft_v2_cer_43000                                     0.215314  0.168142\n",
      "ft_v3_cer_1000                                      0.233648  0.175689\n",
      "ft_v3_cer_25000                                     0.222136  0.171305\n",
      "ft_v4_cer_22000                                     0.226609  0.171463\n",
      "ft_v5_base_cer_28000                                0.215194  0.160661\n",
      "ft_v6_ft_cer_25000                                  0.220910  0.169975\n",
      "ft_v7_scratch_cer_23000                             0.218550  0.167179\n",
      "ft_v8_scratch_cer_10000                             0.242408  0.165445\n",
      "ft_v8_scratch_cer_55000                             0.220250  0.163733\n",
      "ft_v8_scratch_cer_59000                             0.230436  0.164733\n",
      "whisper-small-latin-added-tibetan-checkpoint-40...  0.298808  0.175807\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_...  0.384848  0.200548\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# Let's analyze the CER columns from the provided data\n",
    "# First, I'll create a pandas DataFrame with the data\n",
    "\n",
    "# These are the column names for CER values\n",
    "cer_columns = [\n",
    "    'base_cer', \n",
    "    'ft_v1_cer_5000', 'ft_v1_cer_10000', 'ft_v1_cer_19000',\n",
    "    'ft_v2_cer_20000', 'ft_v2_cer_32000', 'ft_v2_cer_43000',\n",
    "    'ft_v3_cer_1000', 'ft_v3_cer_25000',\n",
    "    'ft_v4_cer_22000', 'ft_v5_base_cer_28000',\n",
    "    'ft_v6_ft_cer_25000', 'ft_v7_scratch_cer_23000',\n",
    "    'ft_v8_scratch_cer_10000', 'ft_v8_scratch_cer_55000', 'ft_v8_scratch_cer_59000',\n",
    "    'whisper-small-latin-added-tibetan-checkpoint-4000_cer',\n",
    "    'whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_cer'\n",
    "]\n",
    "\n",
    "# Let's calculate mean and std for the CER columns\n",
    "df = pd.read_csv('processed_test_with_cer_and_lev_dist.csv')\n",
    "\n",
    "# Calculate and display mean and std for each CER column\n",
    "cer_stats = pd.DataFrame({\n",
    "    'mean': df[cer_columns].mean(),\n",
    "    'std': df[cer_columns].std()\n",
    "})\n",
    "\n",
    "print(cer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         mean        std\n",
      "base_lev_dist                                       19.172452  21.793657\n",
      "ft_v1_lev_dist_5000                                 18.441209  19.909328\n",
      "ft_v1_lev_dist_10000                                15.658455  17.421535\n",
      "ft_v1_lev_dist_19000                                15.333707  16.935004\n",
      "ft_v2_lev_dist_20000                                14.883539  16.348705\n",
      "ft_v2_lev_dist_32000                                14.730123  16.143831\n",
      "ft_v2_lev_dist_43000                                14.608063  16.028661\n",
      "ft_v3_lev_dist_1000                                 15.440090  16.876830\n",
      "ft_v3_lev_dist_25000                                14.991041  16.404976\n",
      "ft_v4_lev_dist_22000                                15.169093  16.709717\n",
      "ft_v5_base_lev_dist_28000                           14.969765  16.644940\n",
      "ft_v6_ft_lev_dist_25000                             15.213886  17.197812\n",
      "ft_v7_scratch_lev_dist_23000                        14.571109  15.466611\n",
      "ft_v8_scratch_lev_dist_10000                        16.829787  19.463031\n",
      "ft_v8_scratch_lev_dist_55000                        15.216125  17.316601\n",
      "ft_v8_scratch_lev_dist_59000                        15.858903  18.459938\n",
      "whisper-small-latin-added-tibetan-checkpoint-40...  23.259798  33.615335\n",
      "whisper-small-tibetan-wylie-checkpoint-4000_to_...  29.949608  43.624014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Let's analyze the CER columns from the provided data\n",
    "# First, I'll create a pandas DataFrame with the data\n",
    "\n",
    "# These are the column names for CER values\n",
    "cer_columns = [\n",
    "    'base_lev_dist', \n",
    "    'ft_v1_lev_dist_5000', 'ft_v1_lev_dist_10000', 'ft_v1_lev_dist_19000',\n",
    "    'ft_v2_lev_dist_20000', 'ft_v2_lev_dist_32000', 'ft_v2_lev_dist_43000',\n",
    "    'ft_v3_lev_dist_1000', 'ft_v3_lev_dist_25000',\n",
    "    'ft_v4_lev_dist_22000', 'ft_v5_base_lev_dist_28000',\n",
    "    'ft_v6_ft_lev_dist_25000', 'ft_v7_scratch_lev_dist_23000',\n",
    "    'ft_v8_scratch_lev_dist_10000', 'ft_v8_scratch_lev_dist_55000', 'ft_v8_scratch_lev_dist_59000',\n",
    "    'whisper-small-latin-added-tibetan-checkpoint-4000_lev_dist',\n",
    "    'whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan_lev_dist'\n",
    "]\n",
    "\n",
    "# Let's calculate mean and std for the CER columns\n",
    "df = pd.read_csv('processed_test_with_cer_and_lev_dist.csv')\n",
    "\n",
    "# Calculate and display mean and std for each CER column\n",
    "cer_stats = pd.DataFrame({\n",
    "    'mean': df[cer_columns].mean(),\n",
    "    'std': df[cer_columns].std()\n",
    "})\n",
    "\n",
    "print(cer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_cer():\n",
    "    # Read the CSV file\n",
    "    print(\"Reading CSV file...\")\n",
    "    df = pd.read_csv('/home/gangagyatso/Downloads/processed_test_with_cer_and_lev_dist (1).csv')\n",
    "\n",
    "    # Define CER columns\n",
    "    cer_columns = [\n",
    "        'base_cer', \n",
    "        'ft_v1_cer_5000', 'ft_v1_cer_10000', 'ft_v1_cer_19000',\n",
    "        'ft_v2_cer_20000', 'ft_v2_cer_32000', 'ft_v2_cer_43000',\n",
    "        'ft_v3_cer_1000', 'ft_v3_cer_25000',\n",
    "        'ft_v4_cer_22000', 'ft_v5_base_cer_28000',\n",
    "        'ft_v6_ft_cer_25000', 'ft_v7_scratch_cer_23000'\n",
    "    ]\n",
    "\n",
    "    # Calculate mean and std for each CER column\n",
    "    means = df[cer_columns].mean()\n",
    "    stds = df[cer_columns].std()\n",
    "\n",
    "    # Print the statistics\n",
    "    print('\\nCER Statistics:')\n",
    "    stats_df = pd.DataFrame({'Mean': means, 'Std Dev': stds})\n",
    "    print(stats_df)\n",
    "\n",
    "    # Create a bar chart with error bars\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    x = np.arange(len(means))\n",
    "    bars = plt.bar(x, means, yerr=stds, align='center', alpha=0.7, capsize=10)\n",
    "    \n",
    "    # Add value labels on top of the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{means.iloc[i]:.3f}', ha='center', va='bottom', \n",
    "                 fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.xticks(x, [col.replace('ft_', '').replace('_cer', '') for col in means.index], \n",
    "              rotation=45, ha='right')\n",
    "    plt.ylabel('Character Error Rate (CER)')\n",
    "    plt.ylim(0, max(means) + max(stds) + 0.1)  # Add padding at the top\n",
    "    plt.title('Mean CER with Standard Deviation Error Bars')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig('cer_comparison.png')\n",
    "    print('Bar chart saved as cer_comparison.png')\n",
    "\n",
    "    # Group by model version and create another visualization\n",
    "    model_groups = {\n",
    "        'base': ['base_cer'],\n",
    "        'v1': ['ft_v1_cer_5000', 'ft_v1_cer_10000', 'ft_v1_cer_19000'],\n",
    "        'v2': ['ft_v2_cer_20000', 'ft_v2_cer_32000', 'ft_v2_cer_43000'],\n",
    "        'v3': ['ft_v3_cer_1000', 'ft_v3_cer_25000'],\n",
    "        'v4': ['ft_v4_cer_22000'],\n",
    "        'v5': ['ft_v5_base_cer_28000'],\n",
    "        'v6': ['ft_v6_ft_cer_25000'],\n",
    "        'v7': ['ft_v7_scratch_cer_23000']\n",
    "    }\n",
    "\n",
    "    # Calculate grouped means and stds\n",
    "    group_means = {}\n",
    "    group_stds = {}\n",
    "\n",
    "    for group, columns in model_groups.items():\n",
    "        group_data = df[columns].values.flatten()\n",
    "        group_means[group] = group_data.mean()\n",
    "        group_stds[group] = group_data.std()\n",
    "\n",
    "    # Create grouped bar chart\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    x = np.arange(len(group_means))\n",
    "    bars = plt.bar(x, list(group_means.values()), yerr=list(group_stds.values()), \n",
    "                  align='center', alpha=0.7, capsize=10, \n",
    "                  color=sns.color_palette(\"muted\", len(group_means)))\n",
    "    \n",
    "    # Add value labels on top of the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{list(group_means.values())[i]:.3f}', ha='center', va='bottom', \n",
    "                 fontweight='bold')\n",
    "    \n",
    "    plt.xticks(x, list(group_means.keys()))\n",
    "    plt.ylabel('Character Error Rate (CER)')\n",
    "    plt.title('Mean CER by Model Version with Standard Deviation')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the grouped plot\n",
    "    plt.savefig('cer_by_model_version.png')\n",
    "    print('Bar chart by model version saved as cer_by_model_version.png')\n",
    "    \n",
    "   # Add histogram for distribution analysis\n",
    "    plt.figure(figsize=(16, 14))  # Increased figure size to accommodate more subplots\n",
    "\n",
    "    # Create subplots for each model version\n",
    "    for i, (group, columns) in enumerate(model_groups.items(), 1):\n",
    "        plt.subplot(3, 3, i)  # Changed from 3x2 to 3x3\n",
    "        for col in columns:\n",
    "            sns.histplot(df[col].dropna(), label=col.replace('ft_', '').replace('_cer', ''), kde=True, alpha=0.6)\n",
    "        plt.title(f'CER Distribution - {group}')\n",
    "        plt.xlabel('CER Value')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cer_distributions.png')\n",
    "    print('CER distributions saved as cer_distributions.png')\n",
    "\n",
    "    # Return the statistics for further analysis\n",
    "    return stats_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_cer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "def analyze_cer():\n",
    "    # Read the CSV file\n",
    "    print(\"Reading CSV file...\")\n",
    "    df = pd.read_csv('processed_test_with_cer_and_lev_dist.csv')\n",
    "\n",
    "    # Define CER columns\n",
    "    cer_columns = [\n",
    "        'base_cer', \n",
    "        'ft_v1_cer_5000', 'ft_v1_cer_10000', 'ft_v1_cer_19000',\n",
    "        'ft_v2_cer_20000', 'ft_v2_cer_32000', 'ft_v2_cer_43000',\n",
    "        'ft_v3_cer_1000', 'ft_v3_cer_25000',\n",
    "        'ft_v4_cer_22000',\n",
    "        'ft_v5_base_cer_28000',\n",
    "        'ft_v6_ft_cer_25000',\n",
    "        'ft_v7_scratch_cer_23000'\n",
    "    ]\n",
    "\n",
    "    # Calculate mean and std for each CER column\n",
    "    means = df[cer_columns].mean()\n",
    "    stds = df[cer_columns].std()\n",
    "\n",
    "    # Print the statistics\n",
    "    print('\\nCER Statistics:')\n",
    "    stats_df = pd.DataFrame({'Mean': means, 'Std Dev': stds})\n",
    "    print(stats_df)\n",
    "\n",
    "    # Create a bar chart with error bars\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    x = np.arange(len(means))\n",
    "    bars = plt.bar(x, means, yerr=stds, align='center', alpha=0.7, capsize=10)\n",
    "    \n",
    "    # Add value labels on top of the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{means.iloc[i]:.3f}', ha='center', va='bottom', \n",
    "                 fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.xticks(x, [col.replace('ft_', '').replace('_cer', '') for col in means.index], \n",
    "              rotation=45, ha='right')\n",
    "    plt.ylabel('Character Error Rate (CER)')\n",
    "    plt.ylim(0, max(means) + max(stds) + 0.1)  # Add padding at the top\n",
    "    plt.title('Mean CER with Standard Deviation Error Bars')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig('cer_comparison.png')\n",
    "    print('Bar chart saved as cer_comparison.png')\n",
    "\n",
    "    # Group by model version and create another visualization\n",
    "    model_groups = {\n",
    "        'base': ['base_cer'],\n",
    "        'v1': ['ft_v1_cer_5000', 'ft_v1_cer_10000', 'ft_v1_cer_19000'],\n",
    "        'v2': ['ft_v2_cer_20000', 'ft_v2_cer_32000', 'ft_v2_cer_43000'],\n",
    "        'v3': ['ft_v3_cer_1000', 'ft_v3_cer_25000'],\n",
    "        'v4': ['ft_v4_cer_22000'],\n",
    "        'v5': ['ft_v5_base_cer_28000'],\n",
    "        'v6': ['ft_v6_ft_cer_25000'],\n",
    "        'v7': ['ft_v7_scratch_cer_23000']\n",
    "    }\n",
    "\n",
    "    # Calculate grouped means and stds\n",
    "    group_means = {}\n",
    "    group_stds = {}\n",
    "\n",
    "    for group, columns in model_groups.items():\n",
    "        group_data = df[columns].values.flatten()\n",
    "        group_means[group] = group_data.mean()\n",
    "        group_stds[group] = group_data.std()\n",
    "\n",
    "    # Create grouped bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(group_means))\n",
    "    bars = plt.bar(x, list(group_means.values()), yerr=list(group_stds.values()), \n",
    "                  align='center', alpha=0.7, capsize=10, \n",
    "                  color=sns.color_palette(\"muted\", len(group_means)))\n",
    "    \n",
    "    # Add value labels on top of the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{list(group_means.values())[i]:.3f}', ha='center', va='bottom', \n",
    "                 fontweight='bold')\n",
    "    \n",
    "    plt.xticks(x, list(group_means.keys()))\n",
    "    plt.ylabel('Character Error Rate (CER)')\n",
    "    plt.title('Mean CER by Model Version with Standard Deviation')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the grouped plot\n",
    "    plt.savefig('cer_by_model_version.png')\n",
    "    print('Bar chart by model version saved as cer_by_model_version.png')\n",
    "    \n",
    "    # Add histogram for distribution analysis\n",
    "    plt.figure(figsize=(16, 14))  # Increased figure size for more subplots\n",
    "    \n",
    "    # Create subplots for each model version\n",
    "    for i, (group, columns) in enumerate(model_groups.items(), 1):\n",
    "        plt.subplot(3, 3, i)  # Changed from 3x2 to 3x3\n",
    "        for col in columns:\n",
    "            sns.histplot(df[col].dropna(), label=col.replace('ft_', '').replace('_cer', ''), kde=True, alpha=0.6)\n",
    "        plt.title(f'CER Distribution - {group}')\n",
    "        plt.xlabel('CER Value')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cer_distributions.png')\n",
    "    print('CER distributions saved as cer_distributions.png')\n",
    "    \n",
    "    # Calculate percentage improvement over base model\n",
    "    base_cer = means['base_cer']\n",
    "    improved_columns = [col for col in cer_columns if col != 'base_cer']\n",
    "    \n",
    "    improvement_percentages = {}\n",
    "    for col in improved_columns:\n",
    "        improvement = (base_cer - means[col]) / base_cer * 100\n",
    "        improvement_percentages[col] = improvement\n",
    "    \n",
    "    # Create a sorted list of columns by model version and checkpoint\n",
    "    # Fixed: Use regex to extract model version and checkpoint number\n",
    "    def extract_version_and_checkpoint(col_name):\n",
    "        # Extract version number (v1, v2, etc)\n",
    "        version_match = re.search(r'v(\\d+)', col_name)\n",
    "        version = int(version_match.group(1)) if version_match else 0\n",
    "        \n",
    "        # Extract checkpoint number (the last number in the string)\n",
    "        checkpoint_match = re.search(r'_(\\d+)$', col_name)\n",
    "        checkpoint = int(checkpoint_match.group(1)) if checkpoint_match else 0\n",
    "        \n",
    "        return (version, checkpoint)\n",
    "    \n",
    "    sorted_cols = sorted(improved_columns, key=extract_version_and_checkpoint)\n",
    "    \n",
    "    # Create improvement visualization\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    x = np.arange(len(sorted_cols))\n",
    "    improvements = [improvement_percentages[col] for col in sorted_cols]\n",
    "    \n",
    "    # Define color mapping for different model versions\n",
    "    version_colors = {\n",
    "        'v1': '#3498db',  # blue\n",
    "        'v2': '#2ecc71',  # green\n",
    "        'v3': '#e74c3c',  # red\n",
    "        'v4': '#9b59b6',  # purple\n",
    "        'v5': '#34495e',  # dark blue\n",
    "        'v6': '#f1c40f',  # yellow\n",
    "        'v7': '#e67e22'   # orange\n",
    "    }\n",
    "    \n",
    "    # Map colors to bars based on model version\n",
    "    colors = [version_colors[re.search(r'v\\d+', col).group(0)] for col in sorted_cols]\n",
    "    \n",
    "    bars = plt.bar(x, improvements, align='center', alpha=0.8, color=colors)\n",
    "    \n",
    "    # Add value labels on top of the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{improvements[i]:.2f}%', ha='center', va='bottom',\n",
    "                fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Format x-axis labels to be more readable\n",
    "    labels = []\n",
    "    for col in sorted_cols:\n",
    "        version = re.search(r'v\\d+', col).group(0)\n",
    "        checkpoint = re.search(r'_(\\d+)$', col).group(1)\n",
    "        labels.append(f\"{version} ({checkpoint})\")\n",
    "    \n",
    "    plt.xticks(x, labels, rotation=45, ha='right')\n",
    "    plt.ylabel('Percentage Improvement Over Base Model (%)')\n",
    "    plt.title('CER Improvement Percentage Over Base Model')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add a horizontal line at zero to emphasize the improvement\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Create a legend for the model versions\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color, label=version) \n",
    "                      for version, color in version_colors.items()]\n",
    "    plt.legend(handles=legend_elements, title=\"Model Version\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cer_improvement_percentage.png', dpi=300, bbox_inches='tight')\n",
    "    print('CER improvement percentage saved as cer_improvement_percentage.png')\n",
    "    \n",
    "    # Create a table with all the important metrics\n",
    "    improvement_df = pd.DataFrame({\n",
    "        'Model': [col.replace('ft_', '').replace('_cer', '') for col in improved_columns],\n",
    "        'CER': [means[col] for col in improved_columns],\n",
    "        'Improvement (%)': [improvement_percentages[col] for col in improved_columns]\n",
    "    })\n",
    "    \n",
    "    improvement_df = improvement_df.sort_values('Improvement (%)', ascending=False)\n",
    "    print(\"\\nCER Improvement Rankings:\")\n",
    "    print(improvement_df)\n",
    "    \n",
    "    # Return the statistics for further analysis\n",
    "    return stats_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_cer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_cer_bins():\n",
    "    \"\"\"\n",
    "    Creates a CSV file with files binned by CER ranges for each model.\n",
    "    \n",
    "    Bins are: 0-0.05, 0.05-0.1, 0.1-0.15, 0.15-0.2, 0.2-0.25, 0.25-0.3, 0.3-0.35,\n",
    "    0.35-0.4, 0.4-0.45, 0.45-0.5, 0.5+\n",
    "    \"\"\"\n",
    "    print(\"Reading CSV file...\")\n",
    "    df = pd.read_csv('processed_test_with_cer_and_lev_dist.csv')\n",
    "\n",
    "    # Define CER columns\n",
    "    cer_columns = [\n",
    "        'base_cer', \n",
    "        'ft_v1_cer_5000', 'ft_v1_cer_10000', 'ft_v1_cer_19000',\n",
    "        'ft_v2_cer_20000', 'ft_v2_cer_32000', 'ft_v2_cer_43000',\n",
    "        'ft_v3_cer_1000', 'ft_v3_cer_25000',\n",
    "        'ft_v4_cer_22000',\n",
    "        'ft_v5_base_cer_28000',\n",
    "        'ft_v6_ft_cer_25000',\n",
    "        'ft_v7_scratch_cer_23000'\n",
    "    ]\n",
    "    \n",
    "    # Define the bins\n",
    "    bin_edges = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 1.0]\n",
    "    bin_labels = [\n",
    "        '0.00-0.05', '0.05-0.10', '0.10-0.15', '0.15-0.20', '0.20-0.25', \n",
    "        '0.25-0.30', '0.30-0.35', '0.35-0.40', '0.40-0.45', '0.45-0.50', '0.50+'\n",
    "    ]\n",
    "    \n",
    "    # Create a dictionary to store binned file names for each CER column\n",
    "    binned_data = {}\n",
    "    \n",
    "    # For each CER column, categorize files into bins\n",
    "    for col in cer_columns:\n",
    "        # Initialize lists for each bin\n",
    "        binned_data[col] = {bin_label: [] for bin_label in bin_labels}\n",
    "        \n",
    "        # Categorize each file based on its CER value\n",
    "        for _, row in df.iterrows():\n",
    "            cer_value = row[col]\n",
    "            \n",
    "            # Skip if CER value is missing\n",
    "            if pd.isna(cer_value):\n",
    "                continue\n",
    "                \n",
    "            # Find the appropriate bin\n",
    "            bin_idx = np.digitize([cer_value], bin_edges)[0] - 1\n",
    "            \n",
    "            # Handle edge case where CER value is exactly 1.0\n",
    "            if bin_idx >= len(bin_labels):\n",
    "                bin_idx = len(bin_labels) - 1\n",
    "                \n",
    "            bin_label = bin_labels[bin_idx]\n",
    "            \n",
    "            # Add file name to the bin\n",
    "            binned_data[col][bin_label].append(row['file_name'])\n",
    "    \n",
    "    # Create detailed bin CSV - shows files in each bin for each model\n",
    "    detailed_rows = []\n",
    "    \n",
    "    for col in cer_columns:\n",
    "        model_name = col.replace('_cer', '')\n",
    "        for bin_label in bin_labels:\n",
    "            files_in_bin = binned_data[col][bin_label]\n",
    "            count = len(files_in_bin)\n",
    "            \n",
    "            # Only add to the CSV if there are files in this bin\n",
    "            if count > 0:\n",
    "                file_list = ';'.join(files_in_bin)\n",
    "                detailed_rows.append({\n",
    "                    'Model': model_name,\n",
    "                    'CER_Range': bin_label,\n",
    "                    'Count': count,\n",
    "                    'File_Names': file_list\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame from the rows\n",
    "    detailed_df = pd.DataFrame(detailed_rows)\n",
    "    detailed_csv_path = 'cer_bins_detailed.csv'\n",
    "    detailed_df.to_csv(detailed_csv_path, index=False)\n",
    "    print(f\"Detailed CER bins saved as {detailed_csv_path}\")\n",
    "    \n",
    "    # Create summary CSV - shows count of files in each bin for each model\n",
    "    summary_data = {}\n",
    "    \n",
    "    # Initialize the dictionary with model names as keys\n",
    "    for col in cer_columns:\n",
    "        model_name = col.replace('_cer', '')\n",
    "        summary_data[model_name] = {}\n",
    "        \n",
    "        # Count files in each bin\n",
    "        for bin_label in bin_labels:\n",
    "            summary_data[model_name][bin_label] = len(binned_data[col][bin_label])\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    summary_df = pd.DataFrame.from_dict(summary_data, orient='index')\n",
    "    summary_csv_path = 'cer_bins_summary.csv'\n",
    "    summary_df.to_csv(summary_csv_path)\n",
    "    print(f\"Summary CER bins saved as {summary_csv_path}\")\n",
    "    \n",
    "    # Create distribution comparison CSV - shows percentage of files in each bin\n",
    "    distribution_data = {}\n",
    "    \n",
    "    # Calculate percentages for each model\n",
    "    for col in cer_columns:\n",
    "        model_name = col.replace('_cer', '')\n",
    "        distribution_data[model_name] = {}\n",
    "        \n",
    "        # Get total number of files with valid CER values for this model\n",
    "        total_files = sum(len(binned_data[col][bin_label]) for bin_label in bin_labels)\n",
    "        \n",
    "        # Calculate percentage for each bin\n",
    "        for bin_label in bin_labels:\n",
    "            count = len(binned_data[col][bin_label])\n",
    "            percentage = (count / total_files * 100) if total_files > 0 else 0\n",
    "            distribution_data[model_name][bin_label] = round(percentage, 2)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    distribution_df = pd.DataFrame.from_dict(distribution_data, orient='index')\n",
    "    distribution_csv_path = 'cer_bins_distribution.csv'\n",
    "    distribution_df.to_csv(distribution_csv_path)\n",
    "    print(f\"Distribution CER bins saved as {distribution_csv_path}\")\n",
    "    \n",
    "    return detailed_df, summary_df, distribution_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_cer_bins()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the summary CSV\n",
    "df = pd.read_csv('cer_bins_summary.csv', index_col=0)\n",
    "\n",
    "# Set up plot style\n",
    "plt.figure(figsize=(16, 7))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Transpose for grouped bar chart (bins on x-axis, models as bars)\n",
    "df_T = df.transpose()\n",
    "\n",
    "# Plot\n",
    "ax = df_T.plot(kind='bar', figsize=(18, 8), width=0.85)\n",
    "plt.title('Count of Files in Each CER Bin for Each Model', fontsize=18)\n",
    "plt.xlabel('CER Bin', fontsize=14)\n",
    "plt.ylabel('Number of Files', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tibetan_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV file...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the Tibetan WER and SER metrics\n",
    "from tibetan_wer.metrics import wer, ser\n",
    "\n",
    "# Load the CSV file\n",
    "print(\"Loading CSV file...\")\n",
    "csv_file = \"processed_test_with_cer_and_lev_dist.csv\"  # Using the file with CER already calculated\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Define the inference columns and their corresponding WER/SER column names\n",
    "inference_columns = [\n",
    "    \"whisper-small-latin-added-tibetan-checkpoint-4000\",\n",
    "    \"whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating WER and SER metrics...\n",
      "Processing whisper-small-latin-added-tibetan-checkpoint-4000...\n",
      "Processing whisper-small-tibetan-wylie-checkpoint-4000_to_tibetan...\n",
      "\n",
      "Results:\n",
      "                                               model  micro_wer  macro_wer  \\\n",
      "0  whisper-small-latin-added-tibetan-checkpoint-4000   0.607723   0.587186   \n",
      "1  whisper-small-tibetan-wylie-checkpoint-4000_to...   0.675397   0.712424   \n",
      "\n",
      "   micro_ser  macro_ser  substitutions  insertions  deletions  \n",
      "0   0.565648    0.56568           7289         543       1478  \n",
      "1   0.616562    0.65628           6741        1561       1846  \n",
      "\n",
      "Results saved to wer_ser_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Calculate WER and SER for each inference column against the ground truth \"uni\" column\n",
    "print(\"Calculating WER and SER metrics...\")\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Loop through each inference column\n",
    "for col in inference_columns:\n",
    "    print(f\"Processing {col}...\")\n",
    "    \n",
    "    # Get predictions and references\n",
    "    predictions = df[col].tolist()\n",
    "    references = df[\"uni\"].tolist()\n",
    "    \n",
    "    # Calculate WER\n",
    "    wer_result = wer(predictions, references)\n",
    "    \n",
    "    # Calculate SER\n",
    "    ser_result = ser(predictions, references)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"model\": col,\n",
    "        \"micro_wer\": wer_result[\"micro_wer\"],\n",
    "        \"macro_wer\": wer_result[\"macro_wer\"],\n",
    "        \"micro_ser\": ser_result[\"micro_ser\"],\n",
    "        \"macro_ser\": ser_result[\"macro_ser\"],\n",
    "        \"substitutions\": ser_result[\"substitutions\"],\n",
    "        \"insertions\": ser_result[\"insertions\"],\n",
    "        \"deletions\": ser_result[\"deletions\"]\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for easier comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nResults:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv(\"wer_ser_results.csv\", index=False)\n",
    "print(\"\\nResults saved to wer_ser_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tibetan-wer matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Use the results data directly\n",
    "data = \"\"\"model,micro_wer,macro_wer,micro_ser,macro_ser,substitutions,insertions,deletions\n",
    "base_transcript,0.5535589116166444,0.5972172117029008,0.49286104866638314,0.5178815698935828,7100,504,508\n",
    "ft_v1_inference_checkpoint-5000,0.5182565475052571,0.5738740985951234,0.48210705389148795,0.5440424343257099,6651,480,804\n",
    "ft_v1_inference_checkpoint-10000,0.40980054801503857,0.4679552413294852,0.39759402150798956,0.479418050592188,5078,652,814\n",
    "ft_v1_inference_checkpoint-19000,0.3941884916841904,0.4592440920690446,0.3864754845373352,0.4704815022591255,4962,637,762\n",
    "ft_v2_inference_checkpoint-20000,0.3919581979226407,0.4510195125772621,0.38428823136278023,0.4665372243930654,4999,665,661\n",
    "ft_v2_inference_checkpoint-32000,0.38641432485821703,0.4429847994193427,0.3797314539157908,0.4655471996665233,4905,638,707\n",
    "ft_v2_inference_checkpoint-43000,0.38099789715159627,0.4375001935072245,0.3755392186645604,0.4607023706973961,4858,654,669\n",
    "ft_v3_inference_checkpoint-1000,0.41247690052889824,0.47391160986923914,0.40148247159608724,0.48557158754267693,5217,654,737\n",
    "ft_v3_inference_checkpoint-25000,0.3911298030969222,0.4519507580126224,0.3827693055471171,0.46346152957429354,4951,649,700\n",
    "ft_v4_inference_checkpoint-22000,0.40890843051041864,0.47620171588845034,0.3924296737347348,0.47617469678609764,5121,600,738\n",
    "ft_v5_base_inference_checkpoint-28000,0.4128592366023068,0.4590233735658298,0.40148247159608724,0.46976534032892286,5254,596,758\n",
    "ft_v6_ft_inference_checkpoint-25000,0.40400178423500926,0.4508872276075435,0.3927334588978674,0.4641250916524783,5059,647,758\n",
    "ft_v7_scratch_inference_checkpoint-23000,0.41171222838208116,0.46476922104444907,0.3968041800838447,0.470994750522651,5270,645,616\n",
    "\"\"\"\n",
    "\n",
    "# Convert the string to a DataFrame\n",
    "from io import StringIO\n",
    "results_df = pd.read_csv(StringIO(data))\n",
    "\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# Clean up model names for better display\n",
    "results_df['model_display'] = results_df['model'].apply(\n",
    "    lambda x: x.replace('_inference_checkpoint-', ' ck-').replace('ft_', 'FT ')\n",
    ")\n",
    "\n",
    "# Extract version and checkpoint numbers for sorting\n",
    "results_df['version'] = results_df['model'].str.extract(r'ft_v(\\d+)').astype(float)\n",
    "results_df.loc[results_df['model'] == 'base_transcript', 'version'] = 0\n",
    "results_df['checkpoint'] = results_df['model'].str.extract(r'checkpoint-(\\d+)').astype(float)\n",
    "results_df.loc[results_df['model'] == 'base_transcript', 'checkpoint'] = 0\n",
    "\n",
    "# Sort by version and checkpoint\n",
    "results_df = results_df.sort_values(['version', 'checkpoint'])\n",
    "\n",
    "# Reset index for better plotting\n",
    "results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "# Set the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# 1. Create a comprehensive visualization with multiple subplots\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Create subplot for WER\n",
    "plt.subplot(2, 2, 1)\n",
    "ax1 = sns.barplot(x='model_display', y='micro_wer', data=results_df, palette='viridis')\n",
    "plt.title('Micro-Average Word Error Rate (WER)', fontsize=14)\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, min(1.0, results_df['micro_wer'].max() * 1.1))\n",
    "for i, p in enumerate(ax1.patches):\n",
    "    ax1.annotate(f'{p.get_height():.3f}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'bottom', fontsize=9)\n",
    "\n",
    "# Create subplot for SER\n",
    "plt.subplot(2, 2, 2)\n",
    "ax2 = sns.barplot(x='model_display', y='micro_ser', data=results_df, palette='viridis')\n",
    "plt.title('Micro-Average Syllable Error Rate (SER)', fontsize=14)\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, min(1.0, results_df['micro_ser'].max() * 1.1))\n",
    "for i, p in enumerate(ax2.patches):\n",
    "    ax2.annotate(f'{p.get_height():.3f}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'bottom', fontsize=9)\n",
    "\n",
    "# Create subplot for error breakdown\n",
    "plt.subplot(2, 2, 3)\n",
    "error_types = pd.melt(results_df, \n",
    "                      id_vars=['model_display'],\n",
    "                      value_vars=['substitutions', 'insertions', 'deletions'],\n",
    "                      var_name='Error_Type', \n",
    "                      value_name='Count')\n",
    "\n",
    "# Create a pivot table for the stacked bar chart\n",
    "error_pivot = error_types.pivot(index='model_display', columns='Error_Type', values='Count')\n",
    "\n",
    "# Plot stacked bar chart\n",
    "error_pivot.plot(kind='bar', stacked=True, ax=plt.gca(), colormap='tab10')\n",
    "plt.title('Error Type Breakdown', fontsize=14)\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='')\n",
    "\n",
    "# Create subplot for version comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "\n",
    "# Prepare data for grouped bar plot\n",
    "metrics_data = pd.melt(results_df, \n",
    "                      id_vars=['model_display', 'version'],\n",
    "                      value_vars=['micro_wer', 'micro_ser'],\n",
    "                      var_name='Metric', \n",
    "                      value_name='Value')\n",
    "metrics_data['Metric'] = metrics_data['Metric'].apply(lambda x: 'WER' if x == 'micro_wer' else 'SER')\n",
    "\n",
    "sns.lineplot(x='model_display', y='Value', hue='Metric', data=metrics_data, \n",
    "            markers=True, dashes=False, palette=['#1f77b4', '#ff7f0e'])\n",
    "plt.title('WER and SER Trend Across Models', fontsize=14)\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0.35, min(1.0, metrics_data['Value'].max() * 1.05))\n",
    "plt.legend(title='')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wer_ser_comparison_all.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('wer_ser_comparison_all.pdf', bbox_inches='tight')\n",
    "\n",
    "# 2. Create a focused comparison of WER and SER\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Prepare data for comparison\n",
    "main_metrics = pd.melt(results_df,\n",
    "                      id_vars=['model_display'],\n",
    "                      value_vars=['micro_wer', 'micro_ser'],\n",
    "                      var_name='Metric',\n",
    "                      value_name='Value')\n",
    "main_metrics['Metric'] = main_metrics['Metric'].apply(lambda x: 'WER' if x == 'micro_wer' else 'SER')\n",
    "\n",
    "# Create grouped bar plot\n",
    "ax = sns.barplot(x='model_display', y='Value', hue='Metric', data=main_metrics, \n",
    "               palette=['#3498db', '#e74c3c'])\n",
    "plt.title('WER and SER Comparison Across Models', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Error Rate', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0.35, min(1.0, main_metrics['Value'].max() * 1.05))\n",
    "plt.legend(title='', loc='upper right', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, p in enumerate(plt.gca().patches):\n",
    "    height = p.get_height()\n",
    "    if not np.isnan(height):\n",
    "        plt.gca().text(p.get_x() + p.get_width()/2., height + 0.005,\n",
    "                      f'{height:.3f}',\n",
    "                      ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wer_ser_main_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('wer_ser_main_comparison.pdf', bbox_inches='tight')\n",
    "\n",
    "# 3. Create a progression plot showing improvement across checkpoints\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Group by version\n",
    "versions = results_df['version'].unique()\n",
    "version_colors = plt.cm.tab10(np.linspace(0, 1, len(versions)))\n",
    "\n",
    "for i, ver in enumerate(versions):\n",
    "    ver_data = results_df[results_df['version'] == ver]\n",
    "    if len(ver_data) > 0:\n",
    "        if ver == 0:  # Base model\n",
    "            plt.axhline(y=ver_data['micro_wer'].values[0], color='red', linestyle='--', \n",
    "                       label=f'Base WER: {ver_data[\"micro_wer\"].values[0]:.3f}')\n",
    "            plt.axhline(y=ver_data['micro_ser'].values[0], color='blue', linestyle='--', \n",
    "                       label=f'Base SER: {ver_data[\"micro_ser\"].values[0]:.3f}')\n",
    "        else:\n",
    "            # Plot WER\n",
    "            plt.plot(ver_data['checkpoint'], ver_data['micro_wer'], \n",
    "                   marker='o', markersize=8, linewidth=2, color=version_colors[i], \n",
    "                   label=f'v{int(ver)} WER')\n",
    "            \n",
    "            # Plot SER with different marker\n",
    "            plt.plot(ver_data['checkpoint'], ver_data['micro_ser'], \n",
    "                   marker='s', markersize=8, linewidth=2, linestyle='--', color=version_colors[i], \n",
    "                   label=f'v{int(ver)} SER')\n",
    "            \n",
    "            # Add annotations\n",
    "            for j, row in ver_data.iterrows():\n",
    "                plt.annotate(f'{row[\"micro_wer\"]:.3f}', \n",
    "                           (row['checkpoint'], row['micro_wer']),\n",
    "                           textcoords=\"offset points\", \n",
    "                           xytext=(0,10), \n",
    "                           ha='center',\n",
    "                           fontsize=9)\n",
    "                plt.annotate(f'{row[\"micro_ser\"]:.3f}', \n",
    "                           (row['checkpoint'], row['micro_ser']),\n",
    "                           textcoords=\"offset points\", \n",
    "                           xytext=(0,-15), \n",
    "                           ha='center',\n",
    "                           fontsize=9)\n",
    "\n",
    "plt.title('WER and SER Progression Across Checkpoints', fontsize=16)\n",
    "plt.xlabel('Checkpoint', fontsize=14)\n",
    "plt.ylabel('Error Rate', fontsize=14)\n",
    "plt.ylim(0.35, 0.6)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wer_ser_progression.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('wer_ser_progression.pdf', bbox_inches='tight')\n",
    "\n",
    "# 4. Improvement over baseline\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Calculate improvement percentage compared to base model\n",
    "base_wer = results_df.loc[results_df['model'] == 'base_transcript', 'micro_wer'].values[0]\n",
    "base_ser = results_df.loc[results_df['model'] == 'base_transcript', 'micro_ser'].values[0]\n",
    "\n",
    "results_df['wer_improvement'] = (base_wer - results_df['micro_wer']) / base_wer * 100\n",
    "results_df['ser_improvement'] = (base_ser - results_df['micro_ser']) / base_ser * 100\n",
    "\n",
    "# Filter out base model for the improvement chart\n",
    "improvement_df = results_df[results_df['model'] != 'base_transcript'].copy()\n",
    "\n",
    "# Create a bar chart for improvement percentage\n",
    "improvement_data = pd.melt(improvement_df,\n",
    "                         id_vars=['model_display'],\n",
    "                         value_vars=['wer_improvement', 'ser_improvement'],\n",
    "                         var_name='Metric',\n",
    "                         value_name='Improvement_Percentage')\n",
    "improvement_data['Metric'] = improvement_data['Metric'].apply(\n",
    "    lambda x: 'WER Improvement' if x == 'wer_improvement' else 'SER Improvement'\n",
    ")\n",
    "\n",
    "ax = sns.barplot(x='model_display', y='Improvement_Percentage', hue='Metric', \n",
    "               data=improvement_data, palette=['#2ecc71', '#9b59b6'])\n",
    "plt.title('Percentage Improvement Over Base Model', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Improvement Percentage (%)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='', fontsize=12)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, p in enumerate(plt.gca().patches):\n",
    "    height = p.get_height()\n",
    "    if not np.isnan(height):\n",
    "        plt.gca().text(p.get_x() + p.get_width()/2., height + 0.5,\n",
    "                      f'{height:.1f}%',\n",
    "                      ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('improvement_over_base.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('improvement_over_base.pdf', bbox_inches='tight')\n",
    "\n",
    "print(\"Visualization complete!\")\n",
    "print(\"Generated files:\")\n",
    "print(\"1. wer_ser_comparison_all.png/pdf - Comprehensive visualization with multiple metrics\")\n",
    "print(\"2. wer_ser_main_comparison.png/pdf - Direct comparison of WER and SER across models\")\n",
    "print(\"3. wer_ser_progression.png/pdf - Progression chart showing improvement across checkpoints\")\n",
    "print(\"4. improvement_over_base.png/pdf - Percentage improvement over base model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the Tibetan WER and SER metrics\n",
    "from tibetan_wer.metrics import wer, ser\n",
    "\n",
    "# Load the CSV file\n",
    "print(\"Loading CSV file...\")\n",
    "csv_file = \"processed_test_with_cer.csv\"  # Using the file with CER already calculated\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Define the inference columns and their corresponding WER/SER column names\n",
    "inference_columns = [\n",
    "    \"ft_v2_inference_checkpoint-20000\",\n",
    "    \"ft_v2_inference_checkpoint-32000\", \n",
    "    \"ft_v2_inference_checkpoint-43000\",\n",
    "    \"ft_v3_inference_checkpoint-1000\",\n",
    "    \"ft_v3_inference_checkpoint-25000\",\n",
    "    \"ft_v4_inference_checkpoint-22000\",\n",
    "    \"ft_v5_base_inference_checkpoint-28000\",\n",
    "    \"ft_v6_ft_inference_checkpoint-25000\",\n",
    "    \"ft_v7_scratch_inference_checkpoint-23000\"\n",
    "]\n",
    "\n",
    "# Create the mappings\n",
    "wer_columns = {col: col.replace(\"inference\", \"wer\") for col in inference_columns}\n",
    "ser_columns = {col: col.replace(\"inference\", \"ser\") for col in inference_columns}\n",
    "\n",
    "# Function to calculate WER\n",
    "def calculate_wer(reference, prediction):\n",
    "    try:\n",
    "        if not reference or not prediction or pd.isna(reference) or pd.isna(prediction):\n",
    "            return np.nan\n",
    "        result = wer([prediction], [reference])\n",
    "        return min(result, 1.0)  # Cap at 1.0 like the CER calculation\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating WER: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Function to calculate SER (returns micro_ser)\n",
    "def calculate_ser(reference, prediction):\n",
    "    try:\n",
    "        if not reference or not prediction or pd.isna(reference) or pd.isna(prediction):\n",
    "            return np.nan\n",
    "        result = ser([prediction], [reference])\n",
    "        return min(result[\"micro_ser\"], 1.0)  # Return micro_ser and cap at 1.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating SER: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Calculate and add WER columns\n",
    "print(\"Calculating WER metrics...\")\n",
    "for inference_col, wer_col in wer_columns.items():\n",
    "    print(f\"  Processing {wer_col}...\")\n",
    "    df[wer_col] = df.apply(\n",
    "        lambda row: calculate_wer(row['uni'], row[inference_col]), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Calculate and add SER columns\n",
    "print(\"Calculating SER metrics...\")\n",
    "for inference_col, ser_col in ser_columns.items():\n",
    "    print(f\"  Processing {ser_col}...\")\n",
    "    df[ser_col] = df.apply(\n",
    "        lambda row: calculate_ser(row['uni'], row[inference_col]), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "output_file = \"processed_test_with_cer_wer_ser.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"WER and SER calculations completed and saved to {output_file}\")\n",
    "\n",
    "# Calculate and display the mean and standard deviation for each metric\n",
    "print(\"\\nMetric Statistics:\")\n",
    "metric_columns = list(wer_columns.values()) + list(ser_columns.values())\n",
    "stats_df = pd.DataFrame({\n",
    "    'Mean': df[metric_columns].mean(),\n",
    "    'Std Dev': df[metric_columns].std()\n",
    "})\n",
    "print(stats_df)\n",
    "\n",
    "# For a single example, show detailed SER breakdown\n",
    "print(\"\\nDetailed SER breakdown for the first row:\")\n",
    "try:\n",
    "    example_ref = df['uni'].iloc[0]\n",
    "    example_pred = df[inference_columns[0]].iloc[0]\n",
    "    \n",
    "    if not pd.isna(example_ref) and not pd.isna(example_pred):\n",
    "        result = ser([example_pred], [example_ref])\n",
    "        print(f'Reference: {example_ref}')\n",
    "        print(f'Prediction: {example_pred}')\n",
    "        print(f'Micro-Average SER Score: {result[\"micro_ser\"]:.3f}')\n",
    "        print(f'Macro-Average SER Score: {result[\"macro_ser\"]:.3f}')\n",
    "        print(f'Substitutions: {result[\"substitutions\"]:.0f}')\n",
    "        print(f'Insertions: {result[\"insertions\"]:.0f}')\n",
    "        print(f'Deletions: {result[\"deletions\"]:.0f}')\n",
    "    else:\n",
    "        print(\"First row contains NaN values, cannot calculate detailed SER.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating detailed SER: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_test.csv\")\n",
    "df2 = pd.read_csv(\"transcription_data_latest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read both dataframes\n",
    "\n",
    "\n",
    "# Create a mapping dictionary from df2\n",
    "transcript_map = dict(zip(df2['file_name'], df2['transcript']))\n",
    "\n",
    "# Create new 'uni' column in df1 by mapping values from df2\n",
    "df['uni'] = df['file_name'].map(transcript_map)\n",
    "\n",
    "# Save the updated df1\n",
    "df.to_csv('processed_test1_with_uni.csv', index=False)\n",
    "\n",
    "print(\"Number of rows in df1:\", len(df))\n",
    "print(\"Number of rows in df2:\", len(df2))\n",
    "print(\"Number of matched transcripts:\", df['uni'].notna().sum())\n",
    "print(\"\\nFirst few rows of updated df1:\")\n",
    "print(df[['file_name', 'uni']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_test1_with_uni.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['uni'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./processed_test1_with_uni_without_na.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./processed_test1_with_uni_without_na.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/gangagyatso/Downloads/processed_test_with_cer_and_lev_dist (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_name', 'path', 'url', 'audio_duration', 'original_id', 'strata',\n",
       "       'age_group', 'duration_category', 'content_type', 'uni',\n",
       "       'base_transcript', 'base_cer', 'base_lev_dist',\n",
       "       'ft_v1_inference_checkpoint-5000', 'ft_v1_cer_5000',\n",
       "       'ft_v1_lev_dist_5000', 'ft_v1_inference_checkpoint-10000',\n",
       "       'ft_v1_cer_10000', 'ft_v1_lev_dist_10000',\n",
       "       'ft_v1_inference_checkpoint-19000', 'ft_v1_cer_19000',\n",
       "       'ft_v1_lev_dist_19000', 'ft_v2_inference_checkpoint-20000',\n",
       "       'ft_v2_cer_20000', 'ft_v2_lev_dist_20000',\n",
       "       'ft_v2_inference_checkpoint-32000', 'ft_v2_cer_32000',\n",
       "       'ft_v2_lev_dist_32000', 'ft_v2_inference_checkpoint-43000',\n",
       "       'ft_v2_cer_43000', 'ft_v2_lev_dist_43000',\n",
       "       'ft_v3_inference_checkpoint-1000', 'ft_v3_cer_1000',\n",
       "       'ft_v3_lev_dist_1000', 'ft_v3_inference_checkpoint-25000',\n",
       "       'ft_v3_cer_25000', 'ft_v3_lev_dist_25000',\n",
       "       'ft_v4_inference_checkpoint-22000', 'ft_v4_cer_22000',\n",
       "       'ft_v4_lev_dist_22000', 'ft_v5_base_inference_checkpoint-28000',\n",
       "       'ft_v5_base_cer_28000', 'ft_v5_base_lev_dist_28000',\n",
       "       'ft_v6_ft_inference_checkpoint-25000', 'ft_v6_ft_cer_25000',\n",
       "       'ft_v6_ft_lev_dist_25000', 'ft_v7_scratch_inference_checkpoint-23000',\n",
       "       'ft_v7_scratch_cer_23000', 'ft_v7_scratch_lev_dist_23000',\n",
       "       'ft_v8_scartch_inference_checkpoint-10000',\n",
       "       'ft_v8_scartch_inference_checkpoint-55000',\n",
       "       'ft_v8_scartch_inference_checkpoint-59000', 'ft_v8_scratch_cer_10000',\n",
       "       'ft_v8_scratch_cer_55000', 'ft_v8_scratch_cer_59000',\n",
       "       'ft_v8_scratch_lev_dist_10000', 'ft_v8_scratch_lev_dist_55000',\n",
       "       'ft_v8_scratch_lev_dist_59000'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>path</th>\n",
       "      <th>url</th>\n",
       "      <th>audio_duration</th>\n",
       "      <th>original_id</th>\n",
       "      <th>strata</th>\n",
       "      <th>age_group</th>\n",
       "      <th>duration_category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>uni</th>\n",
       "      <th>...</th>\n",
       "      <th>ft_v7_scratch_lev_dist_23000</th>\n",
       "      <th>ft_v8_scartch_inference_checkpoint-10000</th>\n",
       "      <th>ft_v8_scartch_inference_checkpoint-55000</th>\n",
       "      <th>ft_v8_scartch_inference_checkpoint-59000</th>\n",
       "      <th>ft_v8_scratch_cer_10000</th>\n",
       "      <th>ft_v8_scratch_cer_55000</th>\n",
       "      <th>ft_v8_scratch_cer_59000</th>\n",
       "      <th>ft_v8_scratch_lev_dist_10000</th>\n",
       "      <th>ft_v8_scratch_lev_dist_55000</th>\n",
       "      <th>ft_v8_scratch_lev_dist_59000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STT_GR_0001_0010_93600_to_106200</td>\n",
       "      <td>./downloads/STT_GR_0001_0010_93600_to_106200.wav</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>12.600</td>\n",
       "      <td>STT_GR_0001</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>ཁོང་དེ་ནས་དགེ་འདུན་ཁྲི་ལ་སྔོན་ཟླ་རེ་བོད་གླེང་ད...</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>ཁོང། དེ་ནས་དགེ་དུས་ཁྲི་ལའི་རྔབ་བདག་རེད།  བུ་གླ...</td>\n",
       "      <td>ཁོང་། དེ་ནས་དགེ་འདུན་ཁྲིི་ང་ད་རེད།  བུ་གླིང་དེ...</td>\n",
       "      <td>ཁོང་་དེ་ས་དགེ་འདུལ་ཁྲིི་ལང་བདག་རེད། བུ་གླིང་དེ...</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.262570</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STT_GR_0033_0105_224320_to_225856</td>\n",
       "      <td>./downloads/STT_GR_0033_0105_224320_to_225856.wav</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>1.536</td>\n",
       "      <td>STT_GR_0033</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>ཕ་གི་ཕར་རང་རེ་རྫས་ཚར་ལེ།</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>ཨ། ཕན་པ་འརེད་ཡོད་རེ། དེ་ལདྟ་ལུ།</td>\n",
       "      <td>ཕན་པ་ཡོད་ན་དེ་ལྟ་ལུ་</td>\n",
       "      <td>ཨ། ཕན་པ་ཡོད་ལ་དེ་ལྟ་ལུ།</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file_name  \\\n",
       "0   STT_GR_0001_0010_93600_to_106200   \n",
       "1  STT_GR_0033_0105_224320_to_225856   \n",
       "\n",
       "                                                path  \\\n",
       "0   ./downloads/STT_GR_0001_0010_93600_to_106200.wav   \n",
       "1  ./downloads/STT_GR_0033_0105_224320_to_225856.wav   \n",
       "\n",
       "                                                 url  audio_duration  \\\n",
       "0  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...          12.600   \n",
       "1  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...           1.536   \n",
       "\n",
       "   original_id                  strata age_group duration_category  \\\n",
       "0  STT_GR_0001  70-80__short__Teaching     70-80             short   \n",
       "1  STT_GR_0033  70-80__short__Teaching     70-80             short   \n",
       "\n",
       "  content_type                                                uni  ...  \\\n",
       "0     Teaching  ཁོང་དེ་ནས་དགེ་འདུན་ཁྲི་ལ་སྔོན་ཟླ་རེ་བོད་གླེང་ད...  ...   \n",
       "1     Teaching                           ཕ་གི་ཕར་རང་རེ་རྫས་ཚར་ལེ།  ...   \n",
       "\n",
       "  ft_v7_scratch_lev_dist_23000  \\\n",
       "0                           41   \n",
       "1                           18   \n",
       "\n",
       "            ft_v8_scartch_inference_checkpoint-10000  \\\n",
       "0  ཁོང། དེ་ནས་དགེ་དུས་ཁྲི་ལའི་རྔབ་བདག་རེད།  བུ་གླ...   \n",
       "1                    ཨ། ཕན་པ་འརེད་ཡོད་རེ། དེ་ལདྟ་ལུ།   \n",
       "\n",
       "            ft_v8_scartch_inference_checkpoint-55000  \\\n",
       "0  ཁོང་། དེ་ནས་དགེ་འདུན་ཁྲིི་ང་ད་རེད།  བུ་གླིང་དེ...   \n",
       "1                               ཕན་པ་ཡོད་ན་དེ་ལྟ་ལུ་   \n",
       "\n",
       "            ft_v8_scartch_inference_checkpoint-59000  ft_v8_scratch_cer_10000  \\\n",
       "0  ཁོང་་དེ་ས་དགེ་འདུལ་ཁྲིི་ལང་བདག་རེད། བུ་གླིང་དེ...                 0.307263   \n",
       "1                            ཨ། ཕན་པ་ཡོད་ལ་དེ་ལྟ་ལུ།                 0.833333   \n",
       "\n",
       "   ft_v8_scratch_cer_55000 ft_v8_scratch_cer_59000  \\\n",
       "0                 0.262570                0.290503   \n",
       "1                 0.708333                0.750000   \n",
       "\n",
       "   ft_v8_scratch_lev_dist_10000  ft_v8_scratch_lev_dist_55000  \\\n",
       "0                            57                            49   \n",
       "1                            20                            17   \n",
       "\n",
       "  ft_v8_scratch_lev_dist_59000  \n",
       "0                           54  \n",
       "1                           18  \n",
       "\n",
       "[2 rows x 58 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  df['file_name', 'url', 'audio_duration', 'original_id', 'path', 'strata',\n",
    "       'age_group', 'duration_category', 'content_type', 'uni',\n",
    "       'base_transcript', 'base_cer', 'ft_v1_inference_checkpoint-5000',\n",
    "       'ft_v1_cer_5000', 'ft_v1_inference_checkpoint-10000', 'ft_v1_cer_10000',\n",
    "       'ft_v1_inference_checkpoint-19000', 'ft_v1_cer_19000',\n",
    "       'ft_v2_inference_checkpoint-20000', 'ft_v2_cer_20000',\n",
    "       'ft_v2_inference_checkpoint-32000', 'ft_v2_cer_32000',\n",
    "       'ft_v2_inference_checkpoint-43000', 'ft_v2_cer_43000',\n",
    "       'ft_v3_inference_checkpoint-1000', 'ft_v3_cer_1000',\n",
    "       'ft_v3_inference_checkpoint-25000', 'ft_v3_cer_25000',\n",
    "       'ft_v4_inference_checkpoint-22000', 'ft_v4_cer_22000',\n",
    "       'ft_v5_base_inference_checkpoint-28000',\n",
    "       'ft_v6_ft_inference_checkpoint-25000',\n",
    "       'ft_v7_scratch_inference_checkpoint-23000', 'ft_v5_base_cer_28000',\n",
    "       'ft_v6_ft_cer_25000', 'ft_v7_scratch_cer_23000', 'base_lev_dist',\n",
    "       'ft_v1_lev_dist_5000', 'ft_v1_lev_dist_10000', 'ft_v1_lev_dist_19000',\n",
    "       'ft_v2_lev_dist_20000', 'ft_v2_lev_dist_32000', 'ft_v2_lev_dist_43000',\n",
    "       'ft_v3_lev_dist_1000', 'ft_v3_lev_dist_25000', 'ft_v4_lev_dist_22000',\n",
    "       'ft_v5_base_lev_dist_28000', 'ft_v6_ft_lev_dist_25000',\n",
    "       'ft_v7_scratch_lev_dist_23000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    # Basic file and audio information\n",
    "    ['file_name', 'path', 'url', 'audio_duration', 'original_id', \n",
    "    \n",
    "    # Metadata columns\n",
    "    'strata', 'age_group', 'duration_category', 'content_type', 'uni',\n",
    "    \n",
    "    # Base model results\n",
    "    'base_transcript', 'base_cer', 'base_lev_dist',\n",
    "    \n",
    "    # Model v1 results (grouped by checkpoint)\n",
    "    'ft_v1_inference_checkpoint-5000', 'ft_v1_cer_5000', 'ft_v1_lev_dist_5000',\n",
    "    'ft_v1_inference_checkpoint-10000', 'ft_v1_cer_10000', 'ft_v1_lev_dist_10000',\n",
    "    'ft_v1_inference_checkpoint-19000', 'ft_v1_cer_19000', 'ft_v1_lev_dist_19000',\n",
    "    \n",
    "    # Model v2 results\n",
    "    'ft_v2_inference_checkpoint-20000', 'ft_v2_cer_20000', 'ft_v2_lev_dist_20000',\n",
    "    'ft_v2_inference_checkpoint-32000', 'ft_v2_cer_32000', 'ft_v2_lev_dist_32000',\n",
    "    'ft_v2_inference_checkpoint-43000', 'ft_v2_cer_43000', 'ft_v2_lev_dist_43000',\n",
    "    \n",
    "    # Model v3 results\n",
    "    'ft_v3_inference_checkpoint-1000', 'ft_v3_cer_1000', 'ft_v3_lev_dist_1000',\n",
    "    'ft_v3_inference_checkpoint-25000', 'ft_v3_cer_25000', 'ft_v3_lev_dist_25000',\n",
    "    \n",
    "    # Model v4 results\n",
    "    'ft_v4_inference_checkpoint-22000', 'ft_v4_cer_22000', 'ft_v4_lev_dist_22000',\n",
    "    \n",
    "    # Model v5 results\n",
    "    'ft_v5_base_inference_checkpoint-28000', 'ft_v5_base_cer_28000', 'ft_v5_base_lev_dist_28000',\n",
    "    \n",
    "    # Model v6 results\n",
    "    'ft_v6_ft_inference_checkpoint-25000', 'ft_v6_ft_cer_25000', 'ft_v6_ft_lev_dist_25000',\n",
    "    \n",
    "    # Model v7 results\n",
    "    'ft_v7_scratch_inference_checkpoint-23000', 'ft_v7_scratch_cer_23000', 'ft_v7_scratch_lev_dist_23000',\n",
    "\n",
    "    # Model v8 results\n",
    "    'ft_v8_scartch_inference_checkpoint-10000', 'ft_v8_scratch_cer_10000', 'ft_v8_scratch_lev_dist_10000',\n",
    "    'ft_v8_scartch_inference_checkpoint-55000', 'ft_v8_scratch_cer_55000', 'ft_v8_scratch_lev_dist_55000',\n",
    "    'ft_v8_scartch_inference_checkpoint-59000', 'ft_v8_scratch_cer_59000', 'ft_v8_scratch_lev_dist_59000',\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "\n",
    "# First login to Hugging Face\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87116daa9a7f4c40bec0933452a17117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948a4d03b14f4d40b81e6fb18c371130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08a5369889d4e4cacf3421064f858fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09c2b50b27b4679a64e0c0ab0d12b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865ce91837c5492295b787d95dd756cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  44%|####3     |  533kB / 1.21MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b13340c5f364696866a531016834beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully pushed to Hugging Face Hub!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Push to the Hugging Face Hub\n",
    "dataset.push_to_hub(\n",
    "    \"ganga4364/garchen_rinpoche_evaluation_results\",  # Change to your desired repo name\n",
    "    private=False,  # Set to True if you want a private dataset\n",
    "    token=None,  # Will use the token from the login step\n",
    "    commit_message=\"Upload Garchen Rinpoche ASR evaluation results\"\n",
    ")\n",
    "\n",
    "print(\"Dataset successfully pushed to Hugging Face Hub!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
