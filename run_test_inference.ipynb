{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363858b46b6841a6a1d5a361ba385f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce216627df041108699940e6fe5eaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "benchmark_segments.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecfcbc5472346e08369d0a7bfdbc294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "full_audio_strata.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3901ed7b333440e1a0000d03338da113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationCastError",
     "evalue": "An error occurred while generating the dataset\n\nAll the data files must have the same columns, but at some point there are 4 new columns ({'ID', 'split_duration_seconds', 'Filename', 'AGE'}) and 6 missing columns ({'original_id', 'url', 'inference_transcript', 'audio_duration', 'age_group', 'file_name'}).\n\nThis happened while the csv dataset builder was generating data using\n\nhf://datasets/ganga4364/garchen_rinpoche_benchmark/full_audio_strata.csv (at revision c0c0be87d61c9f0ec289dfd617898d6e444e0941)\n\nPlease either edit the data files to have matching columns, or separate them into different configurations (see docs at https://hf.co/docs/hub/datasets-manual-configuration#multiple-configurations)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCastError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/builder.py:1831\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1831\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/arrow_writer.py:644\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    643\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mcombine_chunks()\n\u001b[0;32m--> 644\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_local_files:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/table.py:2272\u001b[0m, in \u001b[0;36mtable_cast\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m!=\u001b[39m schema:\n\u001b[0;32m-> 2272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m!=\u001b[39m schema\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/table.py:2218\u001b[0m, in \u001b[0;36mcast_table_to_schema\u001b[0;34m(table, schema)\u001b[0m\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m table_column_names \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(schema\u001b[38;5;241m.\u001b[39mnames):\n\u001b[0;32m-> 2218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table\u001b[38;5;241m.\u001b[39mschema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbecause column names don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2220\u001b[0m         table_column_names\u001b[38;5;241m=\u001b[39mtable\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m   2221\u001b[0m         requested_column_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[1;32m   2222\u001b[0m     )\n\u001b[1;32m   2223\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2224\u001b[0m     cast_array_to_feature(\n\u001b[1;32m   2225\u001b[0m         table[name] \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m table_column_names \u001b[38;5;28;01melse\u001b[39;00m pa\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(table), \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mschema\u001b[38;5;241m.\u001b[39mfield(name)\u001b[38;5;241m.\u001b[39mtype),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2229\u001b[0m ]\n",
      "\u001b[0;31mCastError\u001b[0m: Couldn't cast\nID: string\nFilename: string\nstrata: string\nAGE: string\nduration_category: string\ncontent_type: string\nsplit_duration_seconds: int64\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 1097\nto\n{'file_name': Value('string'), 'url': Value('string'), 'inference_transcript': Value('string'), 'audio_duration': Value('float64'), 'original_id': Value('string'), 'strata': Value('string'), 'age_group': Value('string'), 'duration_category': Value('string'), 'content_type': Value('string')}\nbecause column names don't match",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDatasetGenerationCastError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mganga4364/garchen_rinpoche_benchmark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m test_df \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      8\u001b[0m test_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/load.py:1412\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1412\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1422\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1423\u001b[0m )\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/builder.py:894\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/builder.py:970\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    973\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    974\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    975\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    976\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m    977\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/builder.py:1702\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1700\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1702\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1703\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1704\u001b[0m     ):\n\u001b[1;32m   1705\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1706\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/datasets/builder.py:1833\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1831\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwrite_table(table)\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n\u001b[0;32m-> 1833\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationCastError\u001b[38;5;241m.\u001b[39mfrom_cast_error(\n\u001b[1;32m   1834\u001b[0m         cast_error\u001b[38;5;241m=\u001b[39mcast_error,\n\u001b[1;32m   1835\u001b[0m         builder_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mbuilder_name,\n\u001b[1;32m   1836\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs,\n\u001b[1;32m   1837\u001b[0m         token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m   1838\u001b[0m     )\n\u001b[1;32m   1839\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(table)\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "\u001b[0;31mDatasetGenerationCastError\u001b[0m: An error occurred while generating the dataset\n\nAll the data files must have the same columns, but at some point there are 4 new columns ({'ID', 'split_duration_seconds', 'Filename', 'AGE'}) and 6 missing columns ({'original_id', 'url', 'inference_transcript', 'audio_duration', 'age_group', 'file_name'}).\n\nThis happened while the csv dataset builder was generating data using\n\nhf://datasets/ganga4364/garchen_rinpoche_benchmark/full_audio_strata.csv (at revision c0c0be87d61c9f0ec289dfd617898d6e444e0941)\n\nPlease either edit the data files to have matching columns, or separate them into different configurations (see docs at https://hf.co/docs/hub/datasets-manual-configuration#multiple-configurations)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"ganga4364/garchen_rinpoche_benchmark\")\n",
    "\n",
    "test_df = dataset[\"train\"].to_pandas()\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset, Audio\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='failed_downloads.log', level=logging.ERROR, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Initialize generator pipelines\n",
    "generator1 = pipeline(task=\"automatic-speech-recognition\", model=\"ganga4364/Garchen_Rinpoche-whisper_latin_tibetan_added_on_uni_Checkpoint-4000\", device=0)\n",
    "generator2 = pipeline(task=\"automatic-speech-recognition\", model=\"ganga4364/Garchen_Rinpoche-wav2vec2-Checkpoint-19000\", device=0)\n",
    "\n",
    "# Function to download and validate audio\n",
    "def download_audio(row):\n",
    "    file_name = os.path.basename(row[\"url\"])\n",
    "    save_path = f\"./downloads/{file_name}\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(save_path):  # Skip if file exists\n",
    "        return save_path\n",
    "\n",
    "    try:\n",
    "        response = requests.get(row[\"url\"], timeout=10)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return save_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {row['file_name']}: {e}\")\n",
    "        return None  # Return None if download failed\n",
    "\n",
    "# Function to process inference in batches\n",
    "def process_inference(batch):\n",
    "    # Perform inference with generator1\n",
    "    results1 = generator1(batch[\"path\"])\n",
    "    batch[\"ganga4364/Garchen_Rinpoche-whisper_latin_tibetan_added_on_uni_Checkpoint-4000_transcript\"] = [result[\"text\"] for result in results1]\n",
    "\n",
    "    # Perform inference with generator2\n",
    "    results2 = generator2(batch[\"path\"])\n",
    "    batch[\"ganga4364/Garchen_Rinpoche-whisper_latin_tibetan_added_on_uni_Checkpoint-4000_transcript\"] = [result[\"text\"] for result in results2]\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to test file\n",
    "input_file = \"benchmark_segments.csv\"\n",
    "output_dir = \"chunks_test\"\n",
    "output_file = \"processed_test.csv\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading audio files: 100%|██████████| 978/978 [00:00<00:00, 55910.33it/s]\n",
      "Parameter 'function'=<function process_inference at 0x7f06b0250af0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14f990259c641f8b150c7744bdd9cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 to chunks_test/chunk_1.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, chunk in enumerate(pd.read_csv(input_file, chunksize=1000), start=1):\n",
    "    if chunk.empty:\n",
    "        print(f\"Chunk {i} is empty. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    chunk_file = os.path.join(output_dir, f\"chunk_{i}.csv\")\n",
    "    if os.path.exists(chunk_file):\n",
    "        print(f\"Skipping chunk {i}, already processed.\")\n",
    "        continue\n",
    "\n",
    "    tqdm.pandas(desc=\"Downloading audio files\")\n",
    "    chunk[\"path\"] = chunk.progress_apply(download_audio, axis=1)\n",
    "    chunk = chunk[chunk[\"path\"].notnull()]  # Remove rows with failed downloads\n",
    "\n",
    "    # Reset index to avoid duplicate field errors\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert to Dataset\n",
    "    dataset = Dataset.from_pandas(chunk)\n",
    "    #dataset = dataset.cast_column(\"path\", Audio())\n",
    "\n",
    "    # Perform batched inference\n",
    "    dataset = dataset.map(process_inference, batched=True, batch_size=8)\n",
    "\n",
    "    # Save processed chunk to CSV\n",
    "    dataset.to_pandas().to_csv(chunk_file, index=False)\n",
    "    print(f\"Saved chunk {i} to {chunk_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks merged and saved to processed_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge all chunk files into final output\n",
    "all_chunks = [pd.read_csv(os.path.join(output_dir, f)) for f in sorted(os.listdir(output_dir)) if f.endswith(\".csv\")]\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"All chunks merged and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_test.csv\")\n",
    "df2 = pd.read_csv(\"transcription_data_latest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>inference_transcript</th>\n",
       "      <th>audio_duration</th>\n",
       "      <th>original_id</th>\n",
       "      <th>strata</th>\n",
       "      <th>age_group</th>\n",
       "      <th>duration_category</th>\n",
       "      <th>content_type</th>\n",
       "      <th>path</th>\n",
       "      <th>inference_checkpoint-10000</th>\n",
       "      <th>inference_checkpoint-19000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STT_GR_0001_0010_93600_to_106200</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>ཁོང་ཨེ་ནས་དགེ་སྡུན་ཁྲད་ི་ལྔ་རྡག་རེད།་བུ་གླིང་ྡ...</td>\n",
       "      <td>12.600</td>\n",
       "      <td>STT_GR_0001</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>./downloads/STT_GR_0001_0010_93600_to_106200.wav</td>\n",
       "      <td>ཁོང་ དེ་ནས་དགེ་འདུན་ཁྲིཡི་རྔ་ཟདག་རེད།  བུ་གླིང...</td>\n",
       "      <td>ཁོང་། དེ་ནས་དགེ་འདུ་ཁྲིཡི་རྔ་བདག་རེད།  བུ་གླིང...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STT_GR_0033_0105_224320_to_225856</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>ཨ་ཕ་པ་རེ་འོ་ར་དེའ་ཐག་ནས་</td>\n",
       "      <td>1.536</td>\n",
       "      <td>STT_GR_0033</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>./downloads/STT_GR_0033_0105_224320_to_225856.wav</td>\n",
       "      <td>ཨ། ཕན་པ་རེ་འོད་ན་དེ་མཚས་ན་</td>\n",
       "      <td>ཨ། ཕན་པ་རེ་འོད་ན་དེ་ཐེང་ལུ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STT_GR_0032_0004_19067_to_24988</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>རུན་པོ་དེ་གས་ན་ཡ་བ་སྐུར་དེ་ལ་གསུམ་དེ་ཁོང་ཁྲོ་ད...</td>\n",
       "      <td>5.921</td>\n",
       "      <td>STT_GR_0032</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>./downloads/STT_GR_0032_0004_19067_to_24988.wav</td>\n",
       "      <td>པུ་སྟེང་ས་ལང་ཡར་སཀུན་ཟླ་ལ་གསམའདེ་ཁོན་འགྲོ་ལྔའ་...</td>\n",
       "      <td>པོ་ྟེ་བས་དི་ཡར་བ་ཀུ་་འདི་ལ་སམ་འདི་སྔོན་འགྲོ་ལྔ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STT_GR_0033_0099_212224_to_213024</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>ཐར་བང་ཟར་དུས།</td>\n",
       "      <td>0.800</td>\n",
       "      <td>STT_GR_0033</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>./downloads/STT_GR_0033_0099_212224_to_213024.wav</td>\n",
       "      <td>ཐར་མ་ང་འྒྱེདས་</td>\n",
       "      <td>ཐག་མ་ཐང་ཅིད</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STT_GR_0033_0025_56251_to_58684</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>ཁོ་དེ་གས་གོ་ཐུབ་ཀི་འདུག ཡག་པོ་འདུག་བསམ་གི་འདུག</td>\n",
       "      <td>2.433</td>\n",
       "      <td>STT_GR_0033</td>\n",
       "      <td>70-80__short__Teaching</td>\n",
       "      <td>70-80</td>\n",
       "      <td>short</td>\n",
       "      <td>Teaching</td>\n",
       "      <td>./downloads/STT_GR_0033_0025_56251_to_58684.wav</td>\n",
       "      <td>ཁོ་ཚོས་གཅིག་གི་གོ་ཐུབ་ཀྱི་འདུག ཡག་པོ་འདུག་བསམ་...</td>\n",
       "      <td>ཁོ་ཚོས་གཅིག་གི་གོ་ཐུབ་ཀྱི་འདུག ཡག་པོ་འདུག་བསམ་...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file_name  \\\n",
       "0   STT_GR_0001_0010_93600_to_106200   \n",
       "1  STT_GR_0033_0105_224320_to_225856   \n",
       "2    STT_GR_0032_0004_19067_to_24988   \n",
       "3  STT_GR_0033_0099_212224_to_213024   \n",
       "4    STT_GR_0033_0025_56251_to_58684   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...   \n",
       "1  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...   \n",
       "2  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...   \n",
       "3  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...   \n",
       "4  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...   \n",
       "\n",
       "                                inference_transcript  audio_duration  \\\n",
       "0  ཁོང་ཨེ་ནས་དགེ་སྡུན་ཁྲད་ི་ལྔ་རྡག་རེད།་བུ་གླིང་ྡ...          12.600   \n",
       "1                          ཨ་ཕ་པ་རེ་འོ་ར་དེའ་ཐག་ནས་            1.536   \n",
       "2  རུན་པོ་དེ་གས་ན་ཡ་བ་སྐུར་དེ་ལ་གསུམ་དེ་ཁོང་ཁྲོ་ད...           5.921   \n",
       "3                                    ཐར་བང་ཟར་དུས།             0.800   \n",
       "4    ཁོ་དེ་གས་གོ་ཐུབ་ཀི་འདུག ཡག་པོ་འདུག་བསམ་གི་འདུག            2.433   \n",
       "\n",
       "   original_id                  strata age_group duration_category  \\\n",
       "0  STT_GR_0001  70-80__short__Teaching     70-80             short   \n",
       "1  STT_GR_0033  70-80__short__Teaching     70-80             short   \n",
       "2  STT_GR_0032  70-80__short__Teaching     70-80             short   \n",
       "3  STT_GR_0033  70-80__short__Teaching     70-80             short   \n",
       "4  STT_GR_0033  70-80__short__Teaching     70-80             short   \n",
       "\n",
       "  content_type                                               path  \\\n",
       "0     Teaching   ./downloads/STT_GR_0001_0010_93600_to_106200.wav   \n",
       "1     Teaching  ./downloads/STT_GR_0033_0105_224320_to_225856.wav   \n",
       "2     Teaching    ./downloads/STT_GR_0032_0004_19067_to_24988.wav   \n",
       "3     Teaching  ./downloads/STT_GR_0033_0099_212224_to_213024.wav   \n",
       "4     Teaching    ./downloads/STT_GR_0033_0025_56251_to_58684.wav   \n",
       "\n",
       "                          inference_checkpoint-10000  \\\n",
       "0  ཁོང་ དེ་ནས་དགེ་འདུན་ཁྲིཡི་རྔ་ཟདག་རེད།  བུ་གླིང...   \n",
       "1                         ཨ། ཕན་པ་རེ་འོད་ན་དེ་མཚས་ན་   \n",
       "2  པུ་སྟེང་ས་ལང་ཡར་སཀུན་ཟླ་ལ་གསམའདེ་ཁོན་འགྲོ་ལྔའ་...   \n",
       "3                                     ཐར་མ་ང་འྒྱེདས་   \n",
       "4  ཁོ་ཚོས་གཅིག་གི་གོ་ཐུབ་ཀྱི་འདུག ཡག་པོ་འདུག་བསམ་...   \n",
       "\n",
       "                          inference_checkpoint-19000  \n",
       "0  ཁོང་། དེ་ནས་དགེ་འདུ་ཁྲིཡི་རྔ་བདག་རེད།  བུ་གླིང...  \n",
       "1                         ཨ། ཕན་པ་རེ་འོད་ན་དེ་ཐེང་ལུ  \n",
       "2  པོ་ྟེ་བས་དི་ཡར་བ་ཀུ་་འདི་ལ་སམ་འདི་སྔོན་འགྲོ་ལྔ...  \n",
       "3                                        ཐག་མ་ཐང་ཅིད  \n",
       "4  ཁོ་ཚོས་གཅིག་གི་གོ་ཐུབ་ཀྱི་འདུག ཡག་པོ་འདུག་བསམ་...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>state</th>\n",
       "      <th>inference_transcript</th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "      <th>created_at</th>\n",
       "      <th>submitted_at</th>\n",
       "      <th>reviewed_at</th>\n",
       "      <th>audio_duration</th>\n",
       "      <th>original_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STT_GR_0001_0003_28900_to_31600</td>\n",
       "      <td>transcribing</td>\n",
       "      <td>ཨེ་ད་རང་གི་</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>2025-06-13 00:49:12.342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7</td>\n",
       "      <td>STT_GR_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STT_GR_0001_0021_176000_to_182100</td>\n",
       "      <td>transcribing</td>\n",
       "      <td>གཉིས་ཙུ་བདུན་པ་ཕྱི་དཔག་ཐམས་ཅད་ཚང་ནས། དེ་ནས་གཉེ...</td>\n",
       "      <td>གཉིས་ཙུ་བདུན་པ་ཕྱི་དཔག་ཐམས་ཅད་ཚང་ནས། དེ་ནས་གཉེ...</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>2025-06-13 00:49:12.342</td>\n",
       "      <td>2025-06-14 20:57:01.034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.1</td>\n",
       "      <td>STT_GR_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STT_GR_0001_0022_182500_to_189800</td>\n",
       "      <td>transcribing</td>\n",
       "      <td>དེ་ནས་སངས་རྒྱས་སེམས་བསྐྱིད་ནས་བཟུང་ཡོད་རེད་དེ།...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>2025-06-13 00:49:12.342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.3</td>\n",
       "      <td>STT_GR_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STT_GR_0001_0023_192100_to_196100</td>\n",
       "      <td>transcribing</td>\n",
       "      <td>དེ་ནས་ཚང་རྒྱལ་ཡོད་རྒྱོ་དེ་གང་རེད་དེན་ན། དཔེར་ན...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>2025-06-13 00:49:12.342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>STT_GR_0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STT_GR_0001_0025_201100_to_204400</td>\n",
       "      <td>transcribing</td>\n",
       "      <td>སང་རྒྱུ་དེ་ག་རེ་བཟང་། རྒྱད་རྒྱུ་དེ་ག་རེ་རྒྱད།</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/wav16k/S...</td>\n",
       "      <td>2025-06-13 00:49:12.342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.3</td>\n",
       "      <td>STT_GR_0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file_name         state  \\\n",
       "0    STT_GR_0001_0003_28900_to_31600  transcribing   \n",
       "1  STT_GR_0001_0021_176000_to_182100  transcribing   \n",
       "2  STT_GR_0001_0022_182500_to_189800  transcribing   \n",
       "3  STT_GR_0001_0023_192100_to_196100  transcribing   \n",
       "4  STT_GR_0001_0025_201100_to_204400  transcribing   \n",
       "\n",
       "                                inference_transcript  \\\n",
       "0                                       ཨེ་ད་རང་གི་    \n",
       "1  གཉིས་ཙུ་བདུན་པ་ཕྱི་དཔག་ཐམས་ཅད་ཚང་ནས། དེ་ནས་གཉེ...   \n",
       "2  དེ་ནས་སངས་རྒྱས་སེམས་བསྐྱིད་ནས་བཟུང་ཡོད་རེད་དེ།...   \n",
       "3  དེ་ནས་ཚང་རྒྱལ་ཡོད་རྒྱོ་དེ་གང་རེད་དེན་ན། དཔེར་ན...   \n",
       "4    སང་རྒྱུ་དེ་ག་རེ་བཟང་། རྒྱད་རྒྱུ་དེ་ག་རེ་རྒྱད།     \n",
       "\n",
       "                                          transcript  \\\n",
       "0                                                NaN   \n",
       "1  གཉིས་ཙུ་བདུན་པ་ཕྱི་དཔག་ཐམས་ཅད་ཚང་ནས། དེ་ནས་གཉེ...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                 url               created_at  \\\n",
       "0  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...  2025-06-13 00:49:12.342   \n",
       "1  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...  2025-06-13 00:49:12.342   \n",
       "2  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...  2025-06-13 00:49:12.342   \n",
       "3  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...  2025-06-13 00:49:12.342   \n",
       "4  https://d38pmlk0v88drf.cloudfront.net/wav16k/S...  2025-06-13 00:49:12.342   \n",
       "\n",
       "              submitted_at  reviewed_at  audio_duration  original_id  \n",
       "0                      NaN          NaN             2.7  STT_GR_0001  \n",
       "1  2025-06-14 20:57:01.034          NaN             6.1  STT_GR_0001  \n",
       "2                      NaN          NaN             7.3  STT_GR_0001  \n",
       "3                      NaN          NaN             4.0  STT_GR_0001  \n",
       "4                      NaN          NaN             3.3  STT_GR_0001  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df1: 978\n",
      "Number of rows in df2: 16431\n",
      "Number of matched transcripts: 749\n",
      "\n",
      "First few rows of updated df1:\n",
      "                           file_name  \\\n",
      "0   STT_GR_0001_0010_93600_to_106200   \n",
      "1  STT_GR_0033_0105_224320_to_225856   \n",
      "2    STT_GR_0032_0004_19067_to_24988   \n",
      "3  STT_GR_0033_0099_212224_to_213024   \n",
      "4    STT_GR_0033_0025_56251_to_58684   \n",
      "\n",
      "                                                 uni  \n",
      "0  ཁོང་དེ་ནས་དགེ་འདུན་ཁྲི་ལ་སྔོན་ཟླ་རེ་བོད་གླེང་ད...  \n",
      "1                           ཕ་གི་ཕར་རང་རེ་རྫས་ཚར་ལེ།  \n",
      "2  དབང་པོ་གསལ་མཁན་དེ་ཡིས་དབང་སྐུར་དེ་ལ་གསང་རྫས་མཁ...  \n",
      "3                                     ཐབས་དང་ཟབ་ཅིང་  \n",
      "4  ཁོ་ཚོས་གཅིག་གོ་ཐུབ་ཀི་འདུག ཡག་པོ་འདུག་བསམ་གི་འ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read both dataframes\n",
    "\n",
    "\n",
    "# Create a mapping dictionary from df2\n",
    "transcript_map = dict(zip(df2['file_name'], df2['transcript']))\n",
    "\n",
    "# Create new 'uni' column in df1 by mapping values from df2\n",
    "df['uni'] = df['file_name'].map(transcript_map)\n",
    "\n",
    "# Save the updated df1\n",
    "df.to_csv('processed_test1_with_uni.csv', index=False)\n",
    "\n",
    "print(\"Number of rows in df1:\", len(df))\n",
    "print(\"Number of rows in df2:\", len(df2))\n",
    "print(\"Number of matched transcripts:\", df['uni'].notna().sum())\n",
    "print(\"\\nFirst few rows of updated df1:\")\n",
    "print(df[['file_name', 'uni']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_test1_with_uni.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['uni'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
